{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import hashlib\n",
    "import re\n",
    "from operator import add\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "import pyspark\n",
    "conf = pyspark.SparkConf().setAll([('spark.executor.memory', '12g'),\\\n",
    "                                   ('spark.executor.cores', '3'),\\\n",
    "                                   ('spark.executor.instances','5'),\\\n",
    "                                   ('spark.driver.memory','200g'),\\\n",
    "                                   ('spark.driver.maxResultSize','200g'),\\\n",
    "                                   (\"spark.local.dir\", \"/data/deng.595/tmp\"),\\\n",
    "                                   (\"spark.sql.shuffle.partitions\",'5000'),\\\n",
    "                                   # (\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.11:2.6.5\"),\\ we use spark-nlp for sentence boundary detection and NER\n",
    "                                   (\"spark.jars\", \"/home/deng.595/anaconda2/envs/py36/lib/python3.7/site-packages/pyspark/jars/spark-nlp_2.11-2.6.5.jar\"), \\\n",
    "                                   (\"spark.jars.packages\", \"com.databricks:spark-xml_2.11:0.11.0\")])\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType\n",
    "\n",
    "from pyspark.sql.types import Row\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create wikidata alias dump and wikipedia link/anchor dump\n",
    "This is used for extra entity detection. In particular, a wikipedia page generally won't link to itself, as such we do string matching to find self mentions. \n",
    "\n",
    "We upload the data that we use, but you can also reprocess with the latest wikidata dump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alias(x):\n",
    "    try:\n",
    "        x = json.loads(x.strip(','))\n",
    "    except:\n",
    "        x = {}\n",
    "    return {\n",
    "        'wikititle':x.get('sitelinks',{}).get('enwiki',{}).get('title',''),\n",
    "        'names':[x.get('labels',{}).get('en', {}).get('value','')]+[alias.get('value', '') for alias in x.get('aliases',{}).get('en', [])]\n",
    "    }\n",
    "wikidata = sc.textFile('/data/deng.595/workspace/wikidata/20201130/wikidata-20201130-all.json.bz2')\\\n",
    "    .filter(lambda x:x!='[' and x!=']')\\\n",
    "    .map(get_alias)\\\n",
    "    .filter(lambda x:x['wikititle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidata.map(lambda x:json.dumps(x)).saveAsTextFile('/data/deng.595/workspace/wikidata/20201130/wikidata_aliases.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidata_aliases = sc.textFile('/data/deng.595/workspace/wikidata/20201130/wikidata_aliases.json/part*')\\\n",
    "                        .map(json.loads).map(lambda x:(x['wikititle'].replace(' ','_'),x['names']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_links = raw_articles.filter(lambda x:'paragraphsWithLinks' in x)\\\n",
    "                            .flatMap(lambda x:[link for p in x['paragraphsWithLinks'] for link in p['links'] if link['anchor'].strip()])\\\n",
    "                            .map(lambda x:((x['id'],x['anchor']),1)).reduceByKey(lambda a,b:a+b)\\\n",
    "                            .map(lambda x:(x[0][0],[(x[0][1],x[1])])).reduceByKey(lambda a,b:a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_aliases = wikipedia_links.map(lambda x:(x[0],{k:v for k,v in x[1]}))\\\n",
    "                    .join(wikidata_aliases.map(lambda x:(x[0],{k:9999 for k in x[1]})))\\\n",
    "                    .map(lambda x:(x[0],x[1][0].update(x[1][1]) or x[1][0]))\\\n",
    "                    .map(lambda x: (x[0], sorted(list(x[1].items()), key=lambda z:(z[1], len(z[0])), reverse=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/deng.595/workspace/wikipedia_dump/raw/wikipedia_link_alias.json', 'r') as f:\n",
    "    json.dump(wikipedia_aliases.collect(), f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/deng.595/workspace/wikipedia_dump/raw/20201201/wikipedia_link_alias.json', 'r') as f:\n",
    "    wikipedia_aliases = sc.parallelize(json.load(f)).map(lambda x:(x[0],x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def filter_aliases_and_compile(x):\n",
    "    final = []\n",
    "    count = 0\n",
    "    for i,(k,v) in enumerate(x):\n",
    "        if len(k.strip())>0:\n",
    "            if v==9999:\n",
    "                final.append([k,re.compile(r'\\b'+re.escape(k)+r'\\b')])\n",
    "                count += 1\n",
    "            elif (i-count)<=50:\n",
    "                final.append([k,re.compile(r'\\b'+re.escape(k)+r'\\b')])\n",
    "    return final\n",
    "wikipedia_aliases = wikipedia_aliases.map(lambda x: (x[0], filter_aliases_and_compile(x[1])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process wikipedia articles\n",
    "Process Json dumps converted from the raw XML dump. Add hyperlinks and do some cleanning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_articles = sc.textFile('/data/deng.595/workspace/wikipedia_dump/raw/20201201/enwiki-20201201-pages-articles.json/*.json').map(lambda x:json.loads(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_article = raw_articles.filter(lambda x: x['type']!='REDIRECT').take(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Ernest Ambrose Vivian, 2nd Baron Swansea (11 February 1848 – 17 July 1922); died unmarried'],\n",
       " ['The Hon. John Aubrey Vivian (23 July 1854 – 1 March 1898); died unmarried'],\n",
       " ['Violet Averil Margaret Vivian (3 December 1871 – 30 March 1943)',\n",
       "  'Henry Hussey Vivian (5 February 1873 – 11 December 1898); died unmarried',\n",
       "  'Odo Richard Vivian, 3rd Baron Swansea (22 April 1875 – 16 November 1934)',\n",
       "  'Averil Vivian (4 December 1876 – 1 February 1959); married George Tryon, 1st Baron Tryon',\n",
       "  'Alexandra Gladys Vivian (c. 1879 – 17 July 1966)',\n",
       "  'Alberta Diana Vivian (10 February 1883 – 1968)',\n",
       "  'a daughter (10 February 1883)']]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_article['lists']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'paragraph': 'Alexandra Gladys Vivian (c. 1879 – 17 July 1966)',\n",
       "  'links': [],\n",
       "  'refs': []},\n",
       " {'paragraph': 'Alberta Diana Vivian (10 February 1883 – 1968)',\n",
       "  'links': [],\n",
       "  'refs': []},\n",
       " {'paragraph': 'a daughter (10 February 1883)', 'links': [], 'refs': []}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_article['paragraphsWithLinks'][-len(test_article['lists']):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'numParagraphs': 2, 'numTables': 0},\n",
       " {'title': 'Biography', 'numParagraphs': 6, 'numTables': 0},\n",
       " {'title': 'Marriages and children', 'numParagraphs': 3, 'numTables': 0},\n",
       " {'title': 'References', 'numParagraphs': 0, 'numTables': 0},\n",
       " {'title': 'Further reading', 'numParagraphs': 0, 'numTables': 0},\n",
       " {'title': 'External links', 'numParagraphs': 2, 'numTables': 0}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_article['sections']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8773523"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_articles.filter(lambda x: x['type']!='REDIRECT').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link paragraph to section\n",
    "# remove paragraphs fall in \"References\", \"Further reading\", \"External links\", \"See also\", \n",
    "# \"Publications\"\n",
    "# remove paragraphs in tables or lists\n",
    "def filter_and_link_paragraphs(paragraphs, sections):\n",
    "    cleaned_paragraphs = []\n",
    "    n_p_acc = 0\n",
    "    for i, section in enumerate(sections):\n",
    "        n_p = section.get('numParagraphs', 0)\n",
    "        if section.get('title', '') not in {\"References\", \"Further reading\", \"External links\", \"See also\", \"Publications\"}:\n",
    "            cleaned_paragraphs += [(p.update({'sec_i':i,'p_i':j}) or p) for j,p in enumerate(paragraphs[n_p_acc:n_p_acc+n_p])]\n",
    "        n_p_acc += n_p\n",
    "    return cleaned_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link with hyperlink and alias lookup\n",
    "def annotate_paragraph_with_link(paragraph_with_links, self_links, self_id):\n",
    "    paragraph = paragraph_with_links['paragraph']\n",
    "    links = paragraph_with_links['links']\n",
    "    paragraph_with_links.pop('refs', None)\n",
    "    for link in links:\n",
    "        link.pop('t', '')\n",
    "    entities = {link['id'] for link in links}\n",
    "    anchors = {link['anchor'] for link in links}\n",
    "    mapped = [False for _ in range(len(paragraph))]\n",
    "    for link in links:\n",
    "        mapped[link['start']:link['end']] = [True]*(link['end']-link['start'])\n",
    "    for alias, pattern in self_links:\n",
    "        if alias not in anchors and alias in paragraph:\n",
    "            tmp = []\n",
    "            for match in pattern.finditer(paragraph):\n",
    "                start = match.start()\n",
    "                end = match.end()\n",
    "                if not any(mapped[start:end]):\n",
    "                    tmp.append({'id':self_id, 'anchor':alias, 'start': start, 'end':end})\n",
    "                    mapped[start:end] = [True]*(end-start)\n",
    "            if tmp:\n",
    "                links += tmp\n",
    "                entities.add(self_id)\n",
    "                anchors.add(alias)\n",
    "    paragraph_with_links['links'] = links\n",
    "    paragraph_with_links['entities'] = list(entities)\n",
    "    return paragraph_with_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only non empty paragraph from non-redirect pages\n",
    "nonredirected_articles_paragraphs = raw_articles.filter(lambda x: x['type']!='REDIRECT')\\\n",
    "    .map(lambda x:(x['wikiTitle'], x)).join(wikipedia_aliases)\\\n",
    "    .map(lambda x:{\n",
    "        'title': x[1][0]['title'],\n",
    "        'wikiTitle': x[1][0]['wikiTitle'],\n",
    "        'wid': x[1][0]['wid'],\n",
    "        'paragraphsWithLinks': [\n",
    "            annotate_paragraph_with_link(y, x[1][1], x[1][0]['wikiTitle'])\\\n",
    "                for i,y in enumerate(filter_and_link_paragraphs(x[1][0]['paragraphsWithLinks'], x[1][0]['sections']))\\\n",
    "                if y['paragraph'].strip()\n",
    "        ]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'The Creation of Adam',\n",
       "  'wikiTitle': 'The_Creation_of_Adam',\n",
       "  'wid': 511187,\n",
       "  'paragraphsWithLinks': [[\"  The Creation of Adam () is a fresco painting by Italian artist Michelangelo, which forms part of the Sistine Chapel's ceiling, painted c. 1508–1512. It illustrates the Biblical creation narrative from the Book of Genesis in which God gives life to Adam, the first man. The fresco is part of a complex iconographic scheme and is chronologically the fourth in the series of panels depicting episodes from Genesis.\",\n",
       "    [{'id': 'fresco', 'anchor': 'fresco', 'start': 31, 'end': 37},\n",
       "     {'id': 'Michelangelo', 'anchor': 'Michelangelo', 'start': 65, 'end': 77},\n",
       "     {'id': 'Sistine_Chapel_ceiling',\n",
       "      'anchor': \"Sistine Chapel's ceiling\",\n",
       "      'start': 103,\n",
       "      'end': 127},\n",
       "     {'id': 'Bible', 'anchor': 'Biblical', 'start': 170, 'end': 178},\n",
       "     {'id': 'Genesis_creation_narrative',\n",
       "      'anchor': 'creation narrative',\n",
       "      'start': 179,\n",
       "      'end': 197},\n",
       "     {'id': 'Book_of_Genesis',\n",
       "      'anchor': 'Book of Genesis',\n",
       "      'start': 207,\n",
       "      'end': 222},\n",
       "     {'id': 'God_the_Father#Christianity',\n",
       "      'anchor': 'God',\n",
       "      'start': 232,\n",
       "      'end': 235},\n",
       "     {'id': 'Adam', 'anchor': 'Adam', 'start': 250, 'end': 254},\n",
       "     {'id': 'man', 'anchor': 'man', 'start': 266, 'end': 269},\n",
       "     {'id': 'The_Creation_of_Adam',\n",
       "      'anchor': 'The Creation of Adam',\n",
       "      'start': 2,\n",
       "      'end': 22}],\n",
       "    ['God_the_Father#Christianity',\n",
       "     'Michelangelo',\n",
       "     'Sistine_Chapel_ceiling',\n",
       "     'Genesis_creation_narrative',\n",
       "     'Adam',\n",
       "     'The_Creation_of_Adam',\n",
       "     'Bible',\n",
       "     'man',\n",
       "     'fresco',\n",
       "     'Book_of_Genesis']],\n",
       "   [\"The image of the near-touching hands of God and Adam has become iconic of humanity. The painting has been reproduced in countless imitations and parodies. Michelangelo's Creation of Adam is one of the most replicated religious paintings of all time.\",\n",
       "    [{'id': 'human', 'anchor': 'humanity.', 'start': 74, 'end': 83},\n",
       "     {'id': 'The_Creation_of_Adam',\n",
       "      'anchor': 'Creation of Adam',\n",
       "      'start': 170,\n",
       "      'end': 186}],\n",
       "    ['The_Creation_of_Adam', 'human']],\n",
       "   [\"  In 1505, Michelangelo was invited back to Rome by the newly elected Pope Julius II. He was commissioned to build the Pope's tomb, which was to include forty statues and be finished in five years.\",\n",
       "    [{'id': 'Sistine_Chapel', 'anchor': '', 'start': 1, 'end': 1},\n",
       "     {'id': 'Pope_Julius_II',\n",
       "      'anchor': 'Pope Julius II',\n",
       "      'start': 70,\n",
       "      'end': 84},\n",
       "     {'id': 'Tomb_of_Pope_Julius_II',\n",
       "      'anchor': \"Pope's tomb\",\n",
       "      'start': 119,\n",
       "      'end': 130}],\n",
       "    ['Pope_Julius_II', 'Sistine_Chapel', 'Tomb_of_Pope_Julius_II']],\n",
       "   ['Under the patronage of the Pope, Michelangelo experienced constant interruptions to his work on the tomb in order to accomplish numerous other tasks. Although Michelangelo worked on the tomb for 40 years, it was never finished to his satisfaction.Goldscheider, pp. 14–16. It is located in the Church of S. Pietro in Vincoli in Rome and is most famous for his central figure of Moses, completed in 1516. Of the other statues intended for the tomb, two known as the Rebellious Slave and the Dying Slave, are now in the Louvre.',\n",
       "    [{'id': 'San_Pietro_in_Vincoli',\n",
       "      'anchor': 'Church of S. Pietro in Vincoli',\n",
       "      'start': 293,\n",
       "      'end': 323},\n",
       "     {'id': 'Moses_(Michelangelo)',\n",
       "      'anchor': 'Moses',\n",
       "      'start': 377,\n",
       "      'end': 382},\n",
       "     {'id': 'Rebellious_Slave',\n",
       "      'anchor': 'Rebellious Slave',\n",
       "      'start': 464,\n",
       "      'end': 480},\n",
       "     {'id': 'Dying_Slave', 'anchor': 'Dying Slave', 'start': 489, 'end': 500},\n",
       "     {'id': 'Louvre_Museum', 'anchor': 'Louvre', 'start': 517, 'end': 523}],\n",
       "    ['Moses_(Michelangelo)',\n",
       "     'San_Pietro_in_Vincoli',\n",
       "     'Louvre_Museum',\n",
       "     'Rebellious_Slave',\n",
       "     'Dying_Slave']],\n",
       "   [\"During the same period, Michelangelo painted the ceiling of the Sistine Chapel, which took approximately four years to complete (1508–1512).Bartz and König, p. 134. According to Condivi's account, Bramante, who was working on the building of  St Peter's Basilica, resented Michelangelo's commission for the Pope's tomb and convinced the Pope to commission him in a medium with which he was unfamiliar, in order that he might fail at the task.Coughlan, p. 112.\",\n",
       "    [{'id': 'Bramante', 'anchor': 'Bramante', 'start': 197, 'end': 205},\n",
       "     {'id': \"St_Peter's_Basilica\",\n",
       "      'anchor': \"St Peter's Basilica\",\n",
       "      'start': 243,\n",
       "      'end': 262}],\n",
       "    [\"St_Peter's_Basilica\", 'Bramante']],\n",
       "   ['Michelangelo was originally commissioned to paint the Twelve Apostles on the triangular pendentives that supported the ceiling, and cover the central part of the ceiling with ornament.Goldscheider, pp. 12–14. Michelangelo persuaded Pope Julius to give him a free hand and proposed a different and more complex scheme, representing the Creation, the Fall of Man, the Promise of Salvation through the prophets, and the genealogy of Christ. The work is part of a larger scheme of decoration within the chapel which represents much of the doctrine of the Catholic Church.',\n",
       "    [{'id': 'Twelve_Apostles',\n",
       "      'anchor': 'Twelve Apostles',\n",
       "      'start': 54,\n",
       "      'end': 69},\n",
       "     {'id': 'Genesis_creation_story',\n",
       "      'anchor': 'Creation',\n",
       "      'start': 335,\n",
       "      'end': 343},\n",
       "     {'id': 'Fall_of_Man', 'anchor': 'Fall of Man', 'start': 349, 'end': 360},\n",
       "     {'id': 'genealogy_of_Christ',\n",
       "      'anchor': 'genealogy of Christ',\n",
       "      'start': 417,\n",
       "      'end': 436}],\n",
       "    ['genealogy_of_Christ',\n",
       "     'Genesis_creation_story',\n",
       "     'Twelve_Apostles',\n",
       "     'Fall_of_Man']],\n",
       "   [\"The composition stretches over 500 square metres of ceiling,Bartz and König, p. 43. and contains over 300 figures.  At its centre are nine episodes from the Book of Genesis, divided into three groups: God's Creation of the Earth; God's Creation of Humankind and their fall from God's grace; and lastly, the state of Humanity as represented by Noah and his family. On the pendentives supporting the ceiling are painted twelve men and women who prophesied the coming of Jesus; seven prophets of Israel and five Sibyls, prophetic women of the Classical world.  Among the most famous paintings on the ceiling are The Creation of Adam, Adam and Eve in the Garden of Eden, the Deluge, the Prophet Jeremiah and the Cumaean Sibyl.\",\n",
       "    [{'id': 'Book_of_Genesis',\n",
       "      'anchor': 'Book of Genesis',\n",
       "      'start': 157,\n",
       "      'end': 172},\n",
       "     {'id': 'Noah', 'anchor': 'Noah', 'start': 343, 'end': 347},\n",
       "     {'id': 'prophet', 'anchor': 'prophet', 'start': 481, 'end': 488},\n",
       "     {'id': 'Sibyl', 'anchor': 'Sibyl', 'start': 509, 'end': 514},\n",
       "     {'id': 'Adam_and_Eve',\n",
       "      'anchor': 'Adam and Eve',\n",
       "      'start': 631,\n",
       "      'end': 643},\n",
       "     {'id': 'Garden_of_Eden',\n",
       "      'anchor': 'Garden of Eden',\n",
       "      'start': 651,\n",
       "      'end': 665},\n",
       "     {'id': 'Deluge_myth', 'anchor': 'Deluge', 'start': 671, 'end': 677},\n",
       "     {'id': 'Jeremiah', 'anchor': 'Jeremiah', 'start': 691, 'end': 699},\n",
       "     {'id': 'Cumaean_Sibyl',\n",
       "      'anchor': 'Cumaean Sibyl',\n",
       "      'start': 708,\n",
       "      'end': 721},\n",
       "     {'id': 'The_Creation_of_Adam',\n",
       "      'anchor': 'The Creation of Adam',\n",
       "      'start': 609,\n",
       "      'end': 629},\n",
       "     {'id': 'The_Creation_of_Adam',\n",
       "      'anchor': 'Creation',\n",
       "      'start': 207,\n",
       "      'end': 215},\n",
       "     {'id': 'The_Creation_of_Adam',\n",
       "      'anchor': 'Creation',\n",
       "      'start': 236,\n",
       "      'end': 244}],\n",
       "    ['Adam_and_Eve',\n",
       "     'Cumaean_Sibyl',\n",
       "     'prophet',\n",
       "     'Sibyl',\n",
       "     'Noah',\n",
       "     'The_Creation_of_Adam',\n",
       "     'Jeremiah',\n",
       "     'Book_of_Genesis',\n",
       "     'Garden_of_Eden',\n",
       "     'Deluge_myth']],\n",
       "   [' God is depicted as an elderly white-bearded caucasian man wrapped in a swirling cloak while Adam, on the lower left, is completely naked. God\\'s right arm is outstretched to impart the spark of life from his own finger into that of Adam, whose left arm is extended in a pose mirroring God\\'s, a reminder that man is created in the image and likeness of God (). Another point is that Adam\\'s finger and God\\'s finger aren\\'t touching. It gives the impression that God, the giver of life, is reaching out to Adam who has yet to receive it; they [God and Adam] are not on \"the same level\" as would be two humans shaking hands, for instance.',\n",
       "    [{'id': 'naked', 'anchor': 'naked', 'start': 132, 'end': 137},\n",
       "     {'id': 'Finger_of_God',\n",
       "      'anchor': 'his own finger',\n",
       "      'start': 204,\n",
       "      'end': 218}],\n",
       "    ['naked', 'Finger_of_God']],\n",
       "   ['Many hypotheses have been formulated regarding the identity and meaning of the twelve figures around God. According to an interpretation that was first proposed by the English art critic Walter Pater (1839–1894) and is now widely accepted, the person protected by God\\'s left arm represents Eve, due to the figure\\'s feminine appearance and gaze towards Adam, and the eleven other figures symbolically represent the souls of Adam and Eve\\'s unborn progeny, the entire human race. This interpretation has been challenged, mainly on the grounds that the Catholic Church regards the teaching of the pre-existence of souls as heretical. Consequently, the figure behind God has also been suggested to be the Virgin Mary, Sophia (the personification of wisdom mentioned in the Book of Wisdom), the personified human soul, or \"an angel of masculine build\".',\n",
       "    [{'id': 'Walter_Pater',\n",
       "      'anchor': 'Walter Pater',\n",
       "      'start': 187,\n",
       "      'end': 199},\n",
       "     {'id': 'Eve', 'anchor': 'Eve', 'start': 290, 'end': 293},\n",
       "     {'id': 'pre-existence',\n",
       "      'anchor': 'pre-existence',\n",
       "      'start': 593,\n",
       "      'end': 606},\n",
       "     {'id': 'Virgin_Mary', 'anchor': 'Virgin Mary', 'start': 700, 'end': 711},\n",
       "     {'id': 'Sophia_(wisdom)', 'anchor': 'Sophia', 'start': 713, 'end': 719},\n",
       "     {'id': 'Book_of_Wisdom',\n",
       "      'anchor': 'Book of Wisdom',\n",
       "      'start': 768,\n",
       "      'end': 782}],\n",
       "    ['pre-existence',\n",
       "     'Walter_Pater',\n",
       "     'Sophia_(wisdom)',\n",
       "     'Virgin_Mary',\n",
       "     'Eve',\n",
       "     'Book_of_Wisdom']],\n",
       "   ['The Creation of Adam is generally thought to depict the excerpt \"God created man in His own image, in the image of God He created him\" (). The inspiration for Michelangelo\\'s treatment of the subject may come from a medieval hymn, \"Veni Creator Spiritus\", which asks the \\'finger of the paternal right hand\\' (digitus paternae dexterae) to give the faithful speech.Veni, Creator Spiritus / Come Holy Spirit, Creator Blest. Preces-latinae.org.',\n",
       "    [{'id': 'medieval', 'anchor': 'medieval', 'start': 215, 'end': 223},\n",
       "     {'id': 'hymn', 'anchor': 'hymn', 'start': 224, 'end': 228},\n",
       "     {'id': 'Veni_Creator_Spiritus',\n",
       "      'anchor': 'Veni Creator Spiritus',\n",
       "      'start': 231,\n",
       "      'end': 252},\n",
       "     {'id': 'The_Creation_of_Adam',\n",
       "      'anchor': 'The Creation of Adam',\n",
       "      'start': 0,\n",
       "      'end': 20}],\n",
       "    ['The_Creation_of_Adam', 'Veni_Creator_Spiritus', 'medieval', 'hymn']],\n",
       "   [\"Michelangelo's main source of inspiration for his Adam in his Creation of Adam may have been a cameo showing a nude Augustus Caesar riding sidesaddle on a Capricorn.Cameo on Google Images. This cameo is now at Alnwick Castle, Northumberland. Alnwick Castle is in Northumberland, not Northampton, as the captions state. The Duke of Northumberland who owns the cameos discussed in the article has expressed his appreciation in a letter to the author. The cameo used to belong to cardinal Domenico Grimani who lived in Rome while Michelangelo painted the ceiling. Evidence suggests that Michelangelo and Grimani were friends. This cameo offers an alternative theory for those scholars who have been dissatisfied with the theory that Michelangelo was mainly inspired by Lorenzo Ghiberti's Adam in his Creation of Adam.Sutherland, Bruce (Winter 2013). pp. 12–18.\",\n",
       "    [{'id': 'Cameo_(carving)', 'anchor': 'cameo', 'start': 95, 'end': 100},\n",
       "     {'id': 'Augustus', 'anchor': 'Augustus Caesar', 'start': 116, 'end': 131},\n",
       "     {'id': 'Capricorn_(astrology)',\n",
       "      'anchor': 'Capricorn',\n",
       "      'start': 155,\n",
       "      'end': 164},\n",
       "     {'id': 'Google_Images',\n",
       "      'anchor': 'Google Images',\n",
       "      'start': 174,\n",
       "      'end': 187},\n",
       "     {'id': 'Alnwick_Castle',\n",
       "      'anchor': 'Alnwick Castle',\n",
       "      'start': 210,\n",
       "      'end': 224},\n",
       "     {'id': 'Northumberland',\n",
       "      'anchor': 'Northumberland',\n",
       "      'start': 226,\n",
       "      'end': 240},\n",
       "     {'id': 'Cardinal_(Catholicism)',\n",
       "      'anchor': 'cardinal',\n",
       "      'start': 477,\n",
       "      'end': 485},\n",
       "     {'id': 'Domenico_Grimani',\n",
       "      'anchor': 'Domenico Grimani',\n",
       "      'start': 486,\n",
       "      'end': 502},\n",
       "     {'id': 'Lorenzo_Ghiberti',\n",
       "      'anchor': 'Lorenzo Ghiberti',\n",
       "      'start': 766,\n",
       "      'end': 782},\n",
       "     {'id': 'The_Creation_of_Adam',\n",
       "      'anchor': 'Creation of Adam',\n",
       "      'start': 62,\n",
       "      'end': 78},\n",
       "     {'id': 'The_Creation_of_Adam',\n",
       "      'anchor': 'Creation of Adam',\n",
       "      'start': 797,\n",
       "      'end': 813}],\n",
       "    ['Google_Images',\n",
       "     'Cardinal_(Catholicism)',\n",
       "     'Domenico_Grimani',\n",
       "     'The_Creation_of_Adam',\n",
       "     'Cameo_(carving)',\n",
       "     'Alnwick_Castle',\n",
       "     'Northumberland',\n",
       "     'Lorenzo_Ghiberti',\n",
       "     'Capricorn_(astrology)',\n",
       "     'Augustus']],\n",
       "   [\"Several hypotheses have been put forward about the meaning of The Creation of Adam's highly original composition, many of them taking Michelangelo's well-documented expertise in human anatomy as their starting point.\",\n",
       "    [{'id': 'The_Creation_of_Adam',\n",
       "      'anchor': 'The Creation of Adam',\n",
       "      'start': 62,\n",
       "      'end': 82}],\n",
       "    ['The_Creation_of_Adam']],\n",
       "   ['In 1990 in Anderson, Indiana, physician Frank  Meshberger noted in the Journal of the American Medical Association that the background figures and shapes portrayed behind the figure of God appeared to be an anatomically accurate picture of the human brain. Pdf. Excerpt on Mental Health & Illness.com. Retrieved 21 September 2010. On close examination, borders in the painting correlate with major sulci of the cerebrum in the inner and outer surface of the brain, the brain stem, the frontal lobe, the basilar artery, the pituitary gland and the optic chiasm.',\n",
       "    [{'id': 'Anderson,_Indiana',\n",
       "      'anchor': 'Anderson, Indiana',\n",
       "      'start': 11,\n",
       "      'end': 28},\n",
       "     {'id': 'physician', 'anchor': 'physician', 'start': 30, 'end': 39},\n",
       "     {'id': 'Journal_of_the_American_Medical_Association',\n",
       "      'anchor': 'Journal of the American Medical Association',\n",
       "      'start': 71,\n",
       "      'end': 114},\n",
       "     {'id': 'human_brain', 'anchor': 'human brain', 'start': 244, 'end': 255},\n",
       "     {'id': 'Sulcus_(neuroanatomy)',\n",
       "      'anchor': 'sulci',\n",
       "      'start': 398,\n",
       "      'end': 403},\n",
       "     {'id': 'telencephalon', 'anchor': 'cerebrum', 'start': 411, 'end': 419},\n",
       "     {'id': 'brain_stem', 'anchor': 'brain stem', 'start': 469, 'end': 479},\n",
       "     {'id': 'frontal_lobe',\n",
       "      'anchor': 'frontal lobe',\n",
       "      'start': 485,\n",
       "      'end': 497},\n",
       "     {'id': 'pituitary_gland',\n",
       "      'anchor': 'pituitary gland',\n",
       "      'start': 523,\n",
       "      'end': 538},\n",
       "     {'id': 'optic_chiasm',\n",
       "      'anchor': 'optic chiasm',\n",
       "      'start': 547,\n",
       "      'end': 559}],\n",
       "    ['Journal_of_the_American_Medical_Association',\n",
       "     'pituitary_gland',\n",
       "     'human_brain',\n",
       "     'frontal_lobe',\n",
       "     'physician',\n",
       "     'brain_stem',\n",
       "     'Sulcus_(neuroanatomy)',\n",
       "     'Anderson,_Indiana',\n",
       "     'telencephalon',\n",
       "     'optic_chiasm']],\n",
       "   ['Alternatively, it has been observed that the red cloth around God has the shape of a human uterus (one art historian has called it a \"uterine mantle\") and that the scarf hanging out, coloured green, could be a newly cut umbilical cord. In 2015 a group of Italian researchers published on Mayo Clinic Proceedings an article where the images of the mantle and the postpartum uterus were overlapped. According to Enrico Bruschini (2004), \"This is an interesting hypothesis that presents the Creation scene as an idealised representation of the physical birth of man (\"The Creation\"). It explains the navel that appears on Adam, which is at first perplexing because he was created, not born of a woman.\"Bruschini, Enrico (2004). p. 112.',\n",
       "    [{'id': 'uterus', 'anchor': 'uterus', 'start': 91, 'end': 97},\n",
       "     {'id': 'umbilical_cord',\n",
       "      'anchor': 'umbilical cord',\n",
       "      'start': 220,\n",
       "      'end': 234},\n",
       "     {'id': 'Mayo_Clinic_Proceedings',\n",
       "      'anchor': 'Mayo Clinic Proceedings',\n",
       "      'start': 288,\n",
       "      'end': 311},\n",
       "     {'id': 'navel', 'anchor': 'navel', 'start': 597, 'end': 602},\n",
       "     {'id': 'Adam', 'anchor': 'Adam', 'start': 619, 'end': 623},\n",
       "     {'id': 'The_Creation_of_Adam',\n",
       "      'anchor': 'Creation',\n",
       "      'start': 488,\n",
       "      'end': 496},\n",
       "     {'id': 'The_Creation_of_Adam',\n",
       "      'anchor': 'Creation',\n",
       "      'start': 569,\n",
       "      'end': 577}],\n",
       "    ['Adam',\n",
       "     'The_Creation_of_Adam',\n",
       "     'uterus',\n",
       "     'navel',\n",
       "     'Mayo_Clinic_Proceedings',\n",
       "     'umbilical_cord']],\n",
       "   ['Additionally, Deivis Campos notes in Clinical Anatomy Journal that the left side of Adam’s torso contains an extra concealed rib. Due to Michelangelo’s in-depth knowledge of human anatomy, he insinuates that this rib outline is intentional, and represents the rib of Eve.',\n",
       "    [{'id': 'Clinical_Anatomy',\n",
       "      'anchor': 'Clinical Anatomy Journal',\n",
       "      'start': 37,\n",
       "      'end': 61},\n",
       "     {'id': 'Adam', 'anchor': 'Adam', 'start': 84, 'end': 88},\n",
       "     {'id': 'torso', 'anchor': 'torso', 'start': 91, 'end': 96},\n",
       "     {'id': 'rib', 'anchor': 'rib', 'start': 125, 'end': 128},\n",
       "     {'id': 'Eve', 'anchor': 'Eve', 'start': 267, 'end': 270}],\n",
       "    ['torso', 'Adam', 'rib', 'Clinical_Anatomy', 'Eve']],\n",
       "   ['Campos suggests that this extra rib inclusion was a way for Michelangelo to represent Adam and Eve being created side by side, which differs from the Catholic tradition that states Eve was created after Adam. There is significant evidence that Michelangelo radically disagreed with many Catholic traditions and had a tumultuous relationship with the commissioner of the ceiling, Pope Julius II. Thus, Campos suggests that the rib inclusion was an intentional way to slight Pope Julius II and the Catholic Church, without having to admit fault, as very few people knew anything about human anatomy at the time and could challenge the piece. However, in the book of Genesis the creation of men and women has two versions: Gen 2, 22 is the creation of Eve from the rib of Adam, but before this, in Gen 1, 27, there is another story when male and female were created side by side: \"So God created mankind in his own image, in the image of God he created them; male and female he created them\". So, at least for this rib, there is no support for the assumption that Michelangelo was against any Catholic tradition.',\n",
       "    [{'id': 'Catholic_Church', 'anchor': 'Catholic', 'start': 150, 'end': 158},\n",
       "     {'id': 'Art_patronage_of_Julius_II',\n",
       "      'anchor': 'Pope Julius II',\n",
       "      'start': 379,\n",
       "      'end': 393}],\n",
       "    ['Catholic_Church', 'Art_patronage_of_Julius_II']],\n",
       "   ['Michelangelo was a prolific draftsman, as he was trained in a Florentine workshop at a dynamic time in the art scene, when paper had become readily available in sufficient quantity. As follows, sketching was the first step in Michelangelo’s artistic process, as it helped him plan his final paintings and sculptural pieces. Thus, Michelangelo’s sketches provide a critical link between his creative vision and final compositions. This is especially evident through his sheets “filled with multiple figures and close studies of human anatomy.',\n",
       "    [{'id': 'Michelangelo', 'anchor': 'Michelangelo', 'start': 0, 'end': 12},\n",
       "     {'id': 'Drafter', 'anchor': 'draftsman', 'start': 28, 'end': 37}],\n",
       "    ['Drafter', 'Michelangelo']],\n",
       "   ['Michelangelo completed two sketches in Rome in preparation for the Creation of Adam scene. They are both on display in the British Museum in London, revealing Michelangelo’s in depth planning process for the Sistine Chapel ceiling composition, and his serious attention to perspective and shadowing.',\n",
       "    [{'id': 'Rome', 'anchor': 'Rome', 'start': 39, 'end': 43},\n",
       "     {'id': 'British_Museum',\n",
       "      'anchor': 'British Museum',\n",
       "      'start': 123,\n",
       "      'end': 137},\n",
       "     {'id': 'London', 'anchor': 'London', 'start': 141, 'end': 147},\n",
       "     {'id': 'Sistine_Chapel_ceiling',\n",
       "      'anchor': 'Sistine Chapel ceiling',\n",
       "      'start': 208,\n",
       "      'end': 230},\n",
       "     {'id': 'The_Creation_of_Adam',\n",
       "      'anchor': 'Creation of Adam',\n",
       "      'start': 67,\n",
       "      'end': 83}],\n",
       "    ['Sistine_Chapel_ceiling',\n",
       "     'The_Creation_of_Adam',\n",
       "     'British_Museum',\n",
       "     'Rome',\n",
       "     'London']],\n",
       "   ['The first, is a Scheme for the Decoration of the Vault of the Sistine Chapel: Studies of Arms and Hands. The right side of the page was sketched in 1508 with black chalk, and is a study of Adam’s limp hand, before it is ignited with the gift of life from God, in the Creation of Adam scene. Michelangelo sketched this over a previous brown, lead point stylus study of the vaulted Sistine Chapel ceiling. The entire composition is 274 millimeters in height and 386 millimeters in width. The second sketch is titled Studies of a Reclining Male Nude: Adam in the Fresco ‘The Creation of Man.’ It was created in 1511 in dark red chalk, over a stylus under drawing. Red chalk was Michelangelo’s preferred medium at this period of time, as it could be shaved to a finer point than black chalk. Michelangelo used this fine point to create a scintillating skin surface, that was unique for this particular sketch, and is not seen in his later works. The recto drawing is 193 millimeters in height and 259 millimeters in width.',\n",
       "    [{'id': 'Adam', 'anchor': 'Adam’s', 'start': 189, 'end': 195},\n",
       "     {'id': 'God', 'anchor': 'God', 'start': 255, 'end': 258},\n",
       "     {'id': 'stylus', 'anchor': 'stylus', 'start': 352, 'end': 358},\n",
       "     {'id': 'Sistine_Chapel_ceiling',\n",
       "      'anchor': 'Sistine Chapel ceiling',\n",
       "      'start': 380,\n",
       "      'end': 402},\n",
       "     {'id': 'The_Creation_of_Adam',\n",
       "      'anchor': 'Creation of Adam',\n",
       "      'start': 267,\n",
       "      'end': 283},\n",
       "     {'id': 'The_Creation_of_Adam',\n",
       "      'anchor': 'Creation',\n",
       "      'start': 572,\n",
       "      'end': 580}],\n",
       "    ['Sistine_Chapel_ceiling',\n",
       "     'stylus',\n",
       "     'Adam',\n",
       "     'The_Creation_of_Adam',\n",
       "     'God']],\n",
       "   ['In the Studies of a Reclining Male Nude: Adam in the Fresco ‘The Creation of Man, Adam is resting on earth, propped up by his forearm, with his thighs spread out and his torso slightly twisted to the side. Michelangelo employed a male model to capture this effortful pose and used his red chalk to develop thick contours, in order to establish a definitive form, so every chapel visitor could clearly recognize the muscular body from standing on the floor, 68 feet below the ceiling.',\n",
       "    [{'id': 'Sistine_Chapel', 'anchor': 'chapel', 'start': 372, 'end': 378},\n",
       "     {'id': 'Sistine_Chapel_ceiling',\n",
       "      'anchor': 'ceiling',\n",
       "      'start': 475,\n",
       "      'end': 482},\n",
       "     {'id': 'The_Creation_of_Adam',\n",
       "      'anchor': 'Creation',\n",
       "      'start': 65,\n",
       "      'end': 73}],\n",
       "    ['The_Creation_of_Adam', 'Sistine_Chapel', 'Sistine_Chapel_ceiling']],\n",
       "   ['In Michelangelo’s final fresco on the ceiling, Adam is physically beautiful, but spiritually still incomplete. The sketch prefaces this story, as it is also incomplete in the sense that the only complete component of the drawing is Adam’s twisted torso. Adam’s other limbs bleed off of the trimmed page in immature form. However, the work is not “unfinished,” as it reached its purpose for Michelangelo, which was to work out the details of the torso in the medium of chalk, so he was confident in the composition when he began the actual, permanent fresco panel.',\n",
       "    [{'id': 'Sistine_Chapel_ceiling',\n",
       "      'anchor': 'ceiling',\n",
       "      'start': 38,\n",
       "      'end': 45},\n",
       "     {'id': 'fresco', 'anchor': 'fresco', 'start': 550, 'end': 556}],\n",
       "    ['fresco', 'Sistine_Chapel_ceiling']],\n",
       "   ['Michelangelo heavily studied the human body and dissected numerous cadavers in his artistic career, and overtime became captivated by the male torso. In his treatises on painting and sculpture, Leon Battista Alberti, defined the male figure as a \"geometrical and harmonious sum of its parts\". Michelangelo however, felt that the torso was the powerhouse of the male body, and therefore warranted significant attention and mass in his art pieces. Thus, the torso in the Study represents an idealization of the male form, “symbolic of the perfection of God’s creation before the fall\".',\n",
       "    [{'id': 'Cadaver', 'anchor': 'cadavers', 'start': 67, 'end': 75},\n",
       "     {'id': 'De_pictura',\n",
       "      'anchor': 'treatises on painting and sculpture',\n",
       "      'start': 157,\n",
       "      'end': 192},\n",
       "     {'id': 'Leon_Battista_Alberti',\n",
       "      'anchor': 'Leon Battista Alberti',\n",
       "      'start': 194,\n",
       "      'end': 215},\n",
       "     {'id': 'Fall_of_man', 'anchor': 'the fall', 'start': 573, 'end': 581}],\n",
       "    ['Fall_of_man', 'Cadaver', 'Leon_Battista_Alberti', 'De_pictura']],\n",
       "   ['Michelangelo’s inspiration for the torso in the Studies of a Reclining Male Nude: Adam in the Fresco ‘The Creation of Man sketch, is believed to be the Belvedere Torso. The Belvedere Torso is a fragmentary marble statue that is a 1st century BC Roman copy of an ancient Greek sculpture. Michelangelo historically used ancient, classical statuary as inspiration for the human physique in his great masterpieces. In 2015, the Belvedere Torso was displayed with Michelangelo’s sketch in the “Defining Beauty: The Body in Ancient Greek Art” show at the British Museum in London.',\n",
       "    [{'id': 'Belvedere_Torso',\n",
       "      'anchor': 'Belvedere Torso',\n",
       "      'start': 152,\n",
       "      'end': 167},\n",
       "     {'id': 'Belvedere_Torso',\n",
       "      'anchor': 'Belvedere Torso',\n",
       "      'start': 173,\n",
       "      'end': 188},\n",
       "     {'id': 'Marble_sculpture',\n",
       "      'anchor': 'marble statue',\n",
       "      'start': 206,\n",
       "      'end': 219},\n",
       "     {'id': 'Belvedere_Torso',\n",
       "      'anchor': 'Belvedere Torso',\n",
       "      'start': 424,\n",
       "      'end': 439},\n",
       "     {'id': 'British_Museum',\n",
       "      'anchor': 'British Museum in London',\n",
       "      'start': 549,\n",
       "      'end': 573},\n",
       "     {'id': 'The_Creation_of_Adam',\n",
       "      'anchor': 'Creation',\n",
       "      'start': 106,\n",
       "      'end': 114}],\n",
       "    ['Marble_sculpture',\n",
       "     'The_Creation_of_Adam',\n",
       "     'British_Museum',\n",
       "     'Belvedere_Torso']]]}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonredirected_articles_paragraphs.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only paragraphs that contain entities (from hyperlink or lookup)\n",
    "nonredirected_articles_paragraphs.map(lambda x:{\n",
    "        'title': x['title'],\n",
    "        'wikiTitle': x['wikiTitle'],\n",
    "        'wid': x['wid'],\n",
    "        'paragraphsWithLinks': [p for p in x['paragraphsWithLinks'] if p['entities']]\n",
    "    }).filter(lambda x:x['paragraphsWithLinks'])\\\n",
    "    .map(lambda x:json.dumps(x)).saveAsTextFile('/data/deng.595/workspace/hybrid_pretrain/data/paragraphs_with_link_cleaned.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonredirected_articles_paragraph = spark.createDataFrame(sc.textFile('/data/deng.595/workspace/hybrid_pretrain/data/paragraphs_with_link_cleaned.json/part-*')\\\n",
    "                                    .map(json.loads)\\\n",
    "                                    .flatMap(lambda x: [Row(\n",
    "                                        title=x['title'],\n",
    "                                        wikiTitle=x['wikiTitle'],\n",
    "                                        wid=x['wid'],\n",
    "                                        sec_i=y['sec_i'],\n",
    "                                        p_i=y['p_i'],\n",
    "                                        p=y['paragraph'],\n",
    "                                        link=y['links']\n",
    "                                    ) for y in x['paragraphsWithLinks']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence boundary detection\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"p\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "sentenceDetector = SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"Sentence\")\n",
    "\n",
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"Sentence\"]) \\\n",
    "    .setCleanAnnotations(False) \\\n",
    "    .setIncludeMetadata(False)\n",
    "\n",
    "pipeline = Pipeline() \\\n",
    "    .setStages([\n",
    "        documentAssembler,\n",
    "        sentenceDetector,\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(nonredirected_articles_paragraph)\n",
    "sentences = model.transform(nonredirected_articles_paragraph).drop('p','document')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- link: array (nullable = true)\n",
      " |    |-- element: map (containsNull = true)\n",
      " |    |    |-- key: string\n",
      " |    |    |-- value: string (valueContainsNull = true)\n",
      " |-- p_i: long (nullable = true)\n",
      " |-- sec_i: long (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- wid: long (nullable = true)\n",
      " |-- wikiTitle: string (nullable = true)\n",
      " |-- Sentence: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add hyperlink infomation to the original sentence\n",
    "def link_sentence(sentences, links):\n",
    "    linked_sentences= [[s.result, [], set(), i] for i,s in enumerate(sentences)]\n",
    "    last = 0\n",
    "    remain_links = 0\n",
    "    pre_sentence_length = 0\n",
    "    links = sorted(links,key=lambda z:int(z['start']))\n",
    "    acc_length = 0\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        s_begin = sentence.begin\n",
    "        s_end = sentence.end+1\n",
    "        length = s_end-s_begin\n",
    "        for j,link in enumerate(links[remain_links:]):\n",
    "            start = int(link['start']) - s_begin\n",
    "            end = int(link['end']) - s_begin\n",
    "            if start>=0 and end<=length:\n",
    "                linked_sentences[i][1].append((link['id'], link['anchor'], start, end))\n",
    "                linked_sentences[i][2].add(link['id'])\n",
    "            elif start>=length or end>length:\n",
    "                remain_links += j\n",
    "                break\n",
    "    linked_sentences = [(x[0], x[1], list(x[2]), x[3]) for x in linked_sentences]\n",
    "    return linked_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "linked_sentences = sentences.rdd.flatMap(lambda x:[{\n",
    "    'wid': x.wid,\n",
    "    'title': x.title,\n",
    "    'wikiTitle': x.wikiTitle,\n",
    "    'linked_sentence': y[:3],\n",
    "    'sec_i': x.sec_i,\n",
    "    'p_i': x.p_i,\n",
    "    's_i': y[3],\n",
    "    'md5': hashlib.md5((x.wikiTitle+'$$%md5%$$'+y[0]).encode()).hexdigest()\n",
    "} for y in link_sentence(x.Sentence, x.link)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'wid': 514202,\n",
       "  'title': 'Okavango River',\n",
       "  'wikiTitle': 'Okavango_River',\n",
       "  'linked_sentence': ('The Okavango River (formerly spelled Okovango or Okovanggo) is a river in southwest Africa.',\n",
       "   [('Okavango_River', 'Okavango River', 4, 18),\n",
       "    ('river', 'river', 65, 70),\n",
       "    ('Africa', 'Africa', 84, 90)],\n",
       "   ['Africa', 'Okavango_River', 'river']),\n",
       "  'sec_i': 0,\n",
       "  'p_i': 1,\n",
       "  's_i': 0,\n",
       "  'md5': 'df10fc79e82de56c073e1d4f5010a3c2'},\n",
       " {'wid': 514202,\n",
       "  'title': 'Okavango River',\n",
       "  'wikiTitle': 'Okavango_River',\n",
       "  'linked_sentence': ('It is the fourth-longest river system in southern Africa, running southeastward for .',\n",
       "   [],\n",
       "   []),\n",
       "  'sec_i': 0,\n",
       "  'p_i': 1,\n",
       "  's_i': 1,\n",
       "  'md5': 'bce72f66f721cc9048b84bb6b53681a5'},\n",
       " {'wid': 514202,\n",
       "  'title': 'Okavango River',\n",
       "  'wikiTitle': 'Okavango_River',\n",
       "  'linked_sentence': ('It begins at 1,300 m altitude in the sandy highlands of Angola, where it is known by the Portuguese name Rio Cubango.',\n",
       "   [('Angola', 'Angola', 56, 62), ('Okavango_River', 'Cubango', 109, 116)],\n",
       "   ['Angola', 'Okavango_River']),\n",
       "  'sec_i': 0,\n",
       "  'p_i': 1,\n",
       "  's_i': 2,\n",
       "  'md5': '257b591027d82fac756a694101431961'},\n",
       " {'wid': 514202,\n",
       "  'title': 'Okavango River',\n",
       "  'wikiTitle': 'Okavango_River',\n",
       "  'linked_sentence': ('Farther south, it forms part of the border between Angola and Namibia, and then flows into Botswana.',\n",
       "   [('Namibia', 'Namibia', 62, 69), ('Botswana', 'Botswana', 91, 99)],\n",
       "   ['Botswana', 'Namibia']),\n",
       "  'sec_i': 0,\n",
       "  'p_i': 1,\n",
       "  's_i': 3,\n",
       "  'md5': '88ac1ffe218d1e164690ae94fdf2b35b'},\n",
       " {'wid': 514202,\n",
       "  'title': 'Okavango River',\n",
       "  'wikiTitle': 'Okavango_River',\n",
       "  'linked_sentence': ('The Okavango does not have an outlet to the sea.',\n",
       "   [('Okavango_River', 'Okavango', 4, 12)],\n",
       "   ['Okavango_River']),\n",
       "  'sec_i': 0,\n",
       "  'p_i': 1,\n",
       "  's_i': 4,\n",
       "  'md5': '756e913c3f5e6dc69712f5468af922cb'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linked_sentences.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "linked_sentences.map(lambda x:json.dumps(x)).saveAsTextFile('/data/deng.595/workspace/hybrid_pretrain/data/sentences_with_link_cleaned.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 907 ms, sys: 349 ms, total: 1.26 s\n",
      "Wall time: 6min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sc.textFile('/data/deng.595/workspace/hybrid_pretrain/data/sentences_with_link_cleaned.json/part-*')\\\n",
    "    .map(json.loads).filter(lambda x:len(x['linked_sentence'][1])>0)\\\n",
    "    .map(json.dumps).repartition(5000).saveAsTextFile('/data/deng.595/workspace/hybrid_pretrain/data/sentences_with_link_cleaned_nonempty.json')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotate the sentences\n",
    "Run NER for extra entity detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linked_sentences = spark.createDataFrame(sc.textFile('/data/deng.595/workspace/hybrid_pretrain/data/sentences_with_link.json/part-0000*')\\\n",
    "                                    .map(json.loads)\\\n",
    "                                    .map(lambda x: Row(\n",
    "                                        title=x['title'],\\\n",
    "                                        wikiTitle=x['wikiTitle'],\\\n",
    "                                        wid=x['wid'],\\\n",
    "                                        s=x['linked_sentence'][0],\\\n",
    "                                        links=x['linked_sentence'][1],\\\n",
    "                                        entities=x['linked_sentence'][2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 8.34 µs\n",
      "glove_100d download started this may take some time.\n",
      "Approximate size to download 145.3 MB\n",
      "[OK!]\n",
      "onto_100 download started this may take some time.\n",
      "Approximate size to download 13.5 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from sparknlp.pretrained import PretrainedPipeline, NerDLModel, BertEmbeddings, WordEmbeddingsModel\n",
    "import sparknlp\n",
    "\n",
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"s\") \\\n",
    "    .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"sentence\"])\\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "embeddings = WordEmbeddingsModel.pretrained(name = \"glove_100d\", lang=\"en\") \\\n",
    "        .setInputCols(\"sentence\", \"token\") \\\n",
    "        .setOutputCol(\"embeddings\") \\\n",
    "\n",
    "ner = NerDLModel.pretrained(\"onto_100\", \"en\") \\\n",
    "        .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\n",
    "        .setOutputCol(\"ner\")\n",
    "\n",
    "nerConverter = NerConverter()\\\n",
    "    .setInputCols([\"sentence\", \"token\", \"ner\"])\\\n",
    "    .setOutputCol(\"ner_chunk\")\n",
    "\n",
    "pipeline = Pipeline() \\\n",
    "    .setStages([\n",
    "        documentAssembler,\n",
    "        tokenizer,\n",
    "        embeddings,\n",
    "        ner,\n",
    "        nerConverter\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(linked_sentences)\n",
    "annotated_sentences = model.transform(linked_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- entities: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- links: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |-- s: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- wid: long (nullable = true)\n",
      " |-- wikiTitle: string (nullable = true)\n",
      " |-- sentence: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      " |-- token: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      " |-- embeddings: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      " |-- ner: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      " |-- ner_chunk: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "annotated_sentences.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ner_links(original_links, entities, ner_links, s_length):\n",
    "    valid_ner = {\"DATE\", \"TIME\", \"PERCENT\", \"MONEY\", \"QUANTITY\", \"CARDINAL\"}\n",
    "    mapped_loc = [False for _ in range(s_length)]\n",
    "    merged_links = []\n",
    "    final_ner_links = []\n",
    "    entities = set(entities)\n",
    "    for link in original_links:\n",
    "        start = int(link[2])\n",
    "        end = int(link[3])\n",
    "        merged_links.append([link[0],link[1],start,end,'hyper','hyper'])\n",
    "        mapped_loc[start:end] = [True]*(end-start)\n",
    "    for link in ner_links:\n",
    "        start = link.begin\n",
    "        end = link.end\n",
    "        ner_type = link.metadata['entity']\n",
    "        e_id = f\"{ner_type}:{link.result}\"\n",
    "        final_ner_links.append([e_id,link.result,start,end,'ner',link.metadata['entity']])\n",
    "        if not any(mapped_loc[start:end]) and ner_type in valid_ner:\n",
    "            merged_links.append([e_id,link.result,start,end,'ner',link.metadata['entity']])\n",
    "            entities.add(e_id)\n",
    "    return [merged_links, list(entities), final_ner_links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_sentences = annotated_sentences\\\n",
    "    .drop('sentence','token','embeddings','ner')\\\n",
    "    .where((F.size(F.col('links'))+F.size(F.col('ner_chunk')))>1)\\\n",
    "    .rdd.map(lambda x:{\n",
    "        'wid': x.wid,\n",
    "        'title': x.title,\n",
    "        'wikiTitle': x.wikiTitle,\n",
    "        's': x.s,\n",
    "        'links': add_ner_links(x.links, x.entities, x.ner_chunk, len(x.s))\n",
    "    })\\\n",
    "    .filter(lambda x:len(x['links'][1])>1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 137 ms, sys: 39.9 ms, total: 177 ms\n",
      "Wall time: 22min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "annotated_sentences.map(lambda x:json.dumps(x)).saveAsTextFile('/data/deng.595/workspace/hybrid_pretrain/data/annotated_sentences.json')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract entities pairs from sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original\n",
    "annotated_sentence = sc.textFile('/data/deng.595/workspace/hybrid_pretrain/data/annotated_sentences/annotated_sentences.json-part-*/part-*')\\\n",
    "                .map(json.loads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'wid': 36909709,\n",
       "  'title': 'Saint Mary Magdalene High School in Poznań',\n",
       "  'wikiTitle': 'Saint_Mary_Magdalene_High_School_in_Poznań',\n",
       "  'sec_i': 0,\n",
       "  'p_i': 0,\n",
       "  's_i': 0,\n",
       "  's': 'Saint Mary Magdalene High School in Poznań (Polish: Liceum Ogólnokształcące św.',\n",
       "  'links': [[['Saint_Mary_Magdalene_High_School_in_Poznań',\n",
       "     'Saint Mary Magdalene High School in Poznań',\n",
       "     0,\n",
       "     42,\n",
       "     'hyper',\n",
       "     'hyper'],\n",
       "    ['Polish_language', 'Polish', 44, 50, 'hyper', 'hyper']],\n",
       "   ['Polish_language', 'Saint_Mary_Magdalene_High_School_in_Poznań'],\n",
       "   [['FAC:Saint Mary Magdalene High School',\n",
       "     'Saint Mary Magdalene High School',\n",
       "     0,\n",
       "     32,\n",
       "     'ner',\n",
       "     'FAC'],\n",
       "    ['GPE:Poznań', 'Poznań', 36, 42, 'ner', 'GPE'],\n",
       "    ['NORP:Polish', 'Polish', 44, 50, 'ner', 'NORP']]]},\n",
       " {'wid': 36909709,\n",
       "  'title': 'Saint Mary Magdalene High School in Poznań',\n",
       "  'wikiTitle': 'Saint_Mary_Magdalene_High_School_in_Poznań',\n",
       "  'sec_i': 0,\n",
       "  'p_i': 0,\n",
       "  's_i': 3,\n",
       "  's': 'colloquially simply as Marynka) is one of the oldest and one of the most prestigious and selective high schools in Poland.',\n",
       "  'links': [[['General_education_liceum',\n",
       "     'high schools',\n",
       "     99,\n",
       "     111,\n",
       "     'hyper',\n",
       "     'hyper'],\n",
       "    ['Poland', 'Poland', 115, 121, 'hyper', 'hyper'],\n",
       "    ['CARDINAL:one', 'one', 35, 38, 'ner', 'CARDINAL'],\n",
       "    ['CARDINAL:one', 'one', 57, 60, 'ner', 'CARDINAL']],\n",
       "   ['Poland', 'General_education_liceum', 'CARDINAL:one'],\n",
       "   [['PERSON:Marynka', 'Marynka', 23, 30, 'ner', 'PERSON'],\n",
       "    ['CARDINAL:one', 'one', 35, 38, 'ner', 'CARDINAL'],\n",
       "    ['CARDINAL:one', 'one', 57, 60, 'ner', 'CARDINAL'],\n",
       "    ['GPE:Poland', 'Poland', 115, 121, 'ner', 'GPE']]]},\n",
       " {'wid': 36909709,\n",
       "  'title': 'Saint Mary Magdalene High School in Poznań',\n",
       "  'wikiTitle': 'Saint_Mary_Magdalene_High_School_in_Poznań',\n",
       "  'sec_i': 0,\n",
       "  'p_i': 0,\n",
       "  's_i': 5,\n",
       "  's': 'Marynka has educated statesmen, scholars and generations of the intelligentsia and has been referred to as \"the chief nurse of Poland\\'s elites\".',\n",
       "  'links': [[['Politician', 'statesmen', 21, 30, 'hyper', 'hyper'],\n",
       "    ['Scholar', 'scholars', 32, 40, 'hyper', 'hyper'],\n",
       "    ['intelligentsia', 'intelligentsia', 64, 78, 'hyper', 'hyper']],\n",
       "   ['Scholar', 'Politician', 'intelligentsia'],\n",
       "   [['PERSON:Marynka', 'Marynka', 0, 7, 'ner', 'PERSON'],\n",
       "    [\"ORG:Poland's\", \"Poland's\", 127, 135, 'ner', 'ORG']]]},\n",
       " {'wid': 36909709,\n",
       "  'title': 'Saint Mary Magdalene High School in Poznań',\n",
       "  'wikiTitle': 'Saint_Mary_Magdalene_High_School_in_Poznań',\n",
       "  'sec_i': 1,\n",
       "  'p_i': 0,\n",
       "  's_i': 1,\n",
       "  's': \"In 1939, in recognition of its importance and to thank the school for educating many generations of best scientists, artists and politicians in its 600 years of history, the school was awarded the Knight's Cross of the Order of Polonia Restituta by the last free Poland's President Ignacy Mościcki.\",\n",
       "  'links': [[['Order_of_Polonia_Restituta',\n",
       "     'Order of Polonia Restituta',\n",
       "     219,\n",
       "     245,\n",
       "     'hyper',\n",
       "     'hyper'],\n",
       "    ['Ignacy_Moscicki', 'Ignacy Mościcki', 282, 297, 'hyper', 'hyper'],\n",
       "    ['DATE:1939', '1939', 3, 7, 'ner', 'DATE'],\n",
       "    ['DATE:600 years', '600 years', 148, 157, 'ner', 'DATE']],\n",
       "   ['DATE:600 years',\n",
       "    'Order_of_Polonia_Restituta',\n",
       "    'Ignacy_Moscicki',\n",
       "    'DATE:1939'],\n",
       "   [['DATE:1939', '1939', 3, 7, 'ner', 'DATE'],\n",
       "    ['DATE:600 years', '600 years', 148, 157, 'ner', 'DATE'],\n",
       "    [\"WORK_OF_ART:the Knight's Cross of the Order of Polonia Restituta\",\n",
       "     \"the Knight's Cross of the Order of Polonia Restituta\",\n",
       "     193,\n",
       "     245,\n",
       "     'ner',\n",
       "     'WORK_OF_ART'],\n",
       "    [\"NORP:Poland's\", \"Poland's\", 263, 271, 'ner', 'NORP'],\n",
       "    ['PERSON:Ignacy Mościcki',\n",
       "     'Ignacy Mościcki',\n",
       "     282,\n",
       "     297,\n",
       "     'ner',\n",
       "     'PERSON']]]},\n",
       " {'wid': 36909709,\n",
       "  'title': 'Saint Mary Magdalene High School in Poznań',\n",
       "  'wikiTitle': 'Saint_Mary_Magdalene_High_School_in_Poznań',\n",
       "  'sec_i': 1,\n",
       "  'p_i': 1,\n",
       "  's_i': 0,\n",
       "  's': \"The schools central administrative building (built in 1853) is located on the East side of Bernardines' Square at the Southern end of the Garbary street.\",\n",
       "  'links': [[['1853', '1853', 54, 58, 'hyper', 'hyper'],\n",
       "    ['Order_of_Friars_Minor', \"Bernardines'\", 91, 103, 'hyper', 'hyper']],\n",
       "   ['Order_of_Friars_Minor', '1853'],\n",
       "   [['DATE:1853', '1853', 54, 58, 'ner', 'DATE'],\n",
       "    ['LOC:East', 'East', 78, 82, 'ner', 'LOC'],\n",
       "    [\"FAC:Bernardines' Square\", \"Bernardines' Square\", 91, 110, 'ner', 'FAC'],\n",
       "    ['LOC:Garbary', 'Garbary', 138, 145, 'ner', 'LOC']]]},\n",
       " {'wid': 36909709,\n",
       "  'title': 'Saint Mary Magdalene High School in Poznań',\n",
       "  'wikiTitle': 'Saint_Mary_Magdalene_High_School_in_Poznań',\n",
       "  'sec_i': 2,\n",
       "  'p_i': 0,\n",
       "  's_i': 0,\n",
       "  's': 'Among its alumni the school counts writers (Jan Kasprowicz, Józef Kościelski) Enigma codebreaker Henryk Zygalski, clergy (Primate of Poland Leon Przyłuski, Nobel Peace Prize Nominee Father Marian Żelazek), six rectors of research universities (including Kazimierz Morawski, rector of Jagiellonian University and candidate for the office of President of Poland in 1922), renowned legal scholars (Prof.',\n",
       "  'links': [[['Jan_Kasprowicz', 'Jan Kasprowicz', 44, 58, 'hyper', 'hyper'],\n",
       "    ['Józef_Kościelski', 'Józef Kościelski', 60, 76, 'hyper', 'hyper'],\n",
       "    ['Enigma_machine', 'Enigma', 78, 84, 'hyper', 'hyper'],\n",
       "    ['Henryk_Zygalski', 'Henryk Zygalski', 97, 112, 'hyper', 'hyper'],\n",
       "    ['Primate_of_Poland', 'Primate of Poland', 122, 139, 'hyper', 'hyper'],\n",
       "    ['Leon_Michał_Przyłuski', 'Leon Przyłuski', 140, 154, 'hyper', 'hyper'],\n",
       "    ['Nobel_Peace_Prize', 'Nobel Peace Prize', 156, 173, 'hyper', 'hyper'],\n",
       "    ['Rector_(academia)', 'rectors', 210, 217, 'hyper', 'hyper'],\n",
       "    ['Kazimierz_Morawski_(philologist)',\n",
       "     'Kazimierz Morawski',\n",
       "     254,\n",
       "     272,\n",
       "     'hyper',\n",
       "     'hyper'],\n",
       "    ['Jagiellonian_University',\n",
       "     'Jagiellonian University',\n",
       "     284,\n",
       "     307,\n",
       "     'hyper',\n",
       "     'hyper'],\n",
       "    ['President_of_Poland', 'President of Poland', 340, 359, 'hyper', 'hyper'],\n",
       "    ['CARDINAL:six', 'six', 206, 209, 'ner', 'CARDINAL'],\n",
       "    ['DATE:1922', '1922', 363, 367, 'ner', 'DATE']],\n",
       "   ['Kazimierz_Morawski_(philologist)',\n",
       "    'Jagiellonian_University',\n",
       "    'CARDINAL:six',\n",
       "    'Primate_of_Poland',\n",
       "    'Enigma_machine',\n",
       "    'Rector_(academia)',\n",
       "    'DATE:1922',\n",
       "    'Jan_Kasprowicz',\n",
       "    'Józef_Kościelski',\n",
       "    'President_of_Poland',\n",
       "    'Henryk_Zygalski',\n",
       "    'Nobel_Peace_Prize',\n",
       "    'Leon_Michał_Przyłuski'],\n",
       "   [['PERSON:Jan Kasprowicz', 'Jan Kasprowicz', 44, 58, 'ner', 'PERSON'],\n",
       "    ['PERSON:Józef Kościelski', 'Józef Kościelski', 60, 76, 'ner', 'PERSON'],\n",
       "    ['PERSON:Enigma', 'Enigma', 78, 84, 'ner', 'PERSON'],\n",
       "    ['PERSON:Henryk Zygalski', 'Henryk Zygalski', 97, 112, 'ner', 'PERSON'],\n",
       "    ['ORG:Primate of Poland', 'Primate of Poland', 122, 139, 'ner', 'ORG'],\n",
       "    ['PERSON:Leon Przyłuski', 'Leon Przyłuski', 140, 154, 'ner', 'PERSON'],\n",
       "    ['WORK_OF_ART:Nobel Peace Prize',\n",
       "     'Nobel Peace Prize',\n",
       "     156,\n",
       "     173,\n",
       "     'ner',\n",
       "     'WORK_OF_ART'],\n",
       "    ['PERSON:Marian', 'Marian', 189, 195, 'ner', 'PERSON'],\n",
       "    ['CARDINAL:six', 'six', 206, 209, 'ner', 'CARDINAL'],\n",
       "    ['PERSON:Kazimierz Morawski',\n",
       "     'Kazimierz Morawski',\n",
       "     254,\n",
       "     272,\n",
       "     'ner',\n",
       "     'PERSON'],\n",
       "    ['ORG:Jagiellonian University',\n",
       "     'Jagiellonian University',\n",
       "     284,\n",
       "     307,\n",
       "     'ner',\n",
       "     'ORG'],\n",
       "    ['GPE:Poland', 'Poland', 353, 359, 'ner', 'GPE'],\n",
       "    ['DATE:1922', '1922', 363, 367, 'ner', 'DATE']]]},\n",
       " {'wid': 36909709,\n",
       "  'title': 'Saint Mary Magdalene High School in Poznań',\n",
       "  'wikiTitle': 'Saint_Mary_Magdalene_High_School_in_Poznań',\n",
       "  'sec_i': 2,\n",
       "  'p_i': 0,\n",
       "  's_i': 1,\n",
       "  's': 'Michał Sczaniecki, Prof. Witalis Ludwiczak), physicians (Prof.',\n",
       "  'links': [[['Michał_Sczaniecki',\n",
       "     'Michał Sczaniecki',\n",
       "     0,\n",
       "     17,\n",
       "     'hyper',\n",
       "     'hyper'],\n",
       "    ['Witalis_Ludwiczak', 'Witalis Ludwiczak', 25, 42, 'hyper', 'hyper']],\n",
       "   ['Witalis_Ludwiczak', 'Michał_Sczaniecki'],\n",
       "   [['PERSON:Michał Sczaniecki', 'Michał Sczaniecki', 0, 17, 'ner', 'PERSON'],\n",
       "    ['PERSON:Prof. Witalis Ludwiczak',\n",
       "     'Prof. Witalis Ludwiczak',\n",
       "     19,\n",
       "     42,\n",
       "     'ner',\n",
       "     'PERSON']]]},\n",
       " {'wid': 36909709,\n",
       "  'title': 'Saint Mary Magdalene High School in Poznań',\n",
       "  'wikiTitle': 'Saint_Mary_Magdalene_High_School_in_Poznań',\n",
       "  'sec_i': 2,\n",
       "  'p_i': 0,\n",
       "  's_i': 2,\n",
       "  's': 'Wiktor Dega, Dr. Karol Marcinkowski) and politicians.Z dziejów Gimnazjum i Liceum św.',\n",
       "  'links': [[['Wiktor_Dega', 'Wiktor Dega', 0, 11, 'hyper', 'hyper'],\n",
       "    ['Karol_Marcinkowski', 'Karol Marcinkowski', 17, 35, 'hyper', 'hyper']],\n",
       "   ['Wiktor_Dega', 'Karol_Marcinkowski'],\n",
       "   [['PERSON:Wiktor Dega', 'Wiktor Dega', 0, 11, 'ner', 'PERSON'],\n",
       "    ['PERSON:Dr. Karol Marcinkowski',\n",
       "     'Dr. Karol Marcinkowski',\n",
       "     13,\n",
       "     35,\n",
       "     'ner',\n",
       "     'PERSON'],\n",
       "    ['PERSON:Gimnazjum i Liceum',\n",
       "     'Gimnazjum i Liceum',\n",
       "     63,\n",
       "     81,\n",
       "     'ner',\n",
       "     'PERSON']]]},\n",
       " {'wid': 40003428,\n",
       "  'title': 'City-level Decoupling: Urban Resource Flows and the Governance of Infrastructure Transitions',\n",
       "  'wikiTitle': 'City-level_Decoupling:_Urban_Resource_Flows_and_the_Governance_of_Infrastructure_Transitions',\n",
       "  'sec_i': 0,\n",
       "  'p_i': 0,\n",
       "  's_i': 0,\n",
       "  's': 'The report Decoupling Natural Resource Use and Environmental Impacts from Economic Growth  is one of a series of reports researched and published by the International Resource Panel (IRP) of the United Nations Environment Programme.',\n",
       "  'links': [[['International_Resource_Panel',\n",
       "     'International Resource Panel',\n",
       "     153,\n",
       "     181,\n",
       "     'hyper',\n",
       "     'hyper'],\n",
       "    ['United_Nations_Environment_Programme',\n",
       "     'United Nations Environment Programme',\n",
       "     195,\n",
       "     231,\n",
       "     'hyper',\n",
       "     'hyper']],\n",
       "   ['International_Resource_Panel', 'United_Nations_Environment_Programme'],\n",
       "   [['WORK_OF_ART:Decoupling Natural Resource Use and Environmental Impacts from Economic Growth',\n",
       "     'Decoupling Natural Resource Use and Environmental Impacts from Economic Growth',\n",
       "     11,\n",
       "     89,\n",
       "     'ner',\n",
       "     'WORK_OF_ART'],\n",
       "    ['ORG:the International Resource Panel',\n",
       "     'the International Resource Panel',\n",
       "     149,\n",
       "     181,\n",
       "     'ner',\n",
       "     'ORG'],\n",
       "    ['ORG:IRP', 'IRP', 183, 186, 'ner', 'ORG'],\n",
       "    ['ORG:the United Nations Environment Programme',\n",
       "     'the United Nations Environment Programme',\n",
       "     191,\n",
       "     231,\n",
       "     'ner',\n",
       "     'ORG']]]},\n",
       " {'wid': 12914193,\n",
       "  'title': 'Salix kusanoi',\n",
       "  'wikiTitle': 'Salix_kusanoi',\n",
       "  'sec_i': 0,\n",
       "  'p_i': 0,\n",
       "  's_i': 0,\n",
       "  's': 'Salix kusanoi is a species of willow in the family Salicaceae.',\n",
       "  'links': [[['Salix_kusanoi', 'Salix kusanoi', 0, 13, 'hyper', 'hyper'],\n",
       "    ['willow', 'willow', 30, 36, 'hyper', 'hyper'],\n",
       "    ['Salicaceae', 'Salicaceae', 51, 61, 'hyper', 'hyper']],\n",
       "   ['willow', 'Salix_kusanoi', 'Salicaceae'],\n",
       "   [['PERSON:Salix kusanoi', 'Salix kusanoi', 0, 13, 'ner', 'PERSON'],\n",
       "    ['FAC:Salicaceae', 'Salicaceae', 51, 61, 'ner', 'FAC']]]}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated_sentence.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplication\n",
    "annotated_sentence.map(lambda x:(hashlib.md5((x['wikiTitle']+'$$%md5%$$'+x['s']).encode()).hexdigest(),x))\\\n",
    "    .reduceByKey(lambda a,b:a if len(a['links'][0])>len(b['links'][0]) else b)\\\n",
    "    .map(lambda x: json.dumps(x[1]))\\\n",
    "    .saveAsTextFile('/data/deng.595/workspace/hybrid_pretrain/data/annotated_sentences_cleaned.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_sentence = sc.textFile('/data/deng.595/workspace/hybrid_pretrain/data/annotated_sentences_cleaned.json/part*')\\\n",
    "                .map(json.loads)\\\n",
    "                .filter(lambda x:len(x['links'][1])>1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence that mentions the topic entity of that Wikipedia page\n",
    "annotated_sentence_only_self = annotated_sentence.filter(lambda x:x['wikiTitle'] in x['links'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_stat = annotated_sentence.map(lambda x:[len(set([y[0] for y in x['links'][0] if y[4]=='hyper'])),\n",
    "                                               len(x['links'][1])]).collect()\n",
    "only_self_stat = annotated_sentence_only_self.map(lambda x:[len(set([y[0] for y in x['links'][0] if y[4]=='hyper'])),\n",
    "                                               len(x['links'][1])]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all 60415414\n",
      ">=3 34420649\n",
      ">=4 19101114\n",
      "average 3.319835696234739\n",
      "max 1253\n",
      "min 2\n",
      ">=2 entity 18911307\n",
      ">=3 entity 9372275\n",
      "average entity 2.3173175309201723\n",
      "max 1253\n",
      "min 1\n"
     ]
    }
   ],
   "source": [
    "print('all', len(overall_stat))\n",
    "print('>=3', len([x for x in overall_stat if x[1]>2]))\n",
    "print('>=4', len([x for x in overall_stat if x[1]>3]))\n",
    "print('average', sum([x[1] for x in overall_stat])/len(overall_stat))\n",
    "print('max', max([x[1] for x in overall_stat]))\n",
    "print('min', min([x[1] for x in overall_stat]))\n",
    "\n",
    "print('>=2 entity', len([x for x in overall_stat if x[0]>2]))\n",
    "print('>=3 entity', len([x for x in overall_stat if x[0]>3]))\n",
    "print('average entity', sum([x[0] for x in overall_stat])/len(overall_stat))\n",
    "print('max', max([x[0] for x in overall_stat]))\n",
    "print('min', min([x[0] for x in overall_stat]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all 16909031\n",
      ">=3 12938247\n",
      ">=4 7949870\n",
      "average 3.860607210430923\n",
      "max 402\n",
      "min 2\n",
      ">=2 entity 8380653\n",
      ">=3 entity 4458609\n",
      "average entity 2.869229999046072\n",
      "max 402\n",
      "min 1\n"
     ]
    }
   ],
   "source": [
    "print('all', len(only_self_stat))\n",
    "print('>=3', len([x for x in only_self_stat if x[1]>2]))\n",
    "print('>=4', len([x for x in only_self_stat if x[1]>3]))\n",
    "print('average', sum([x[1] for x in only_self_stat])/len(only_self_stat))\n",
    "print('max', max([x[1] for x in only_self_stat]))\n",
    "print('min', min([x[1] for x in only_self_stat]))\n",
    "\n",
    "print('>=2 entity', len([x for x in only_self_stat if x[0]>2]))\n",
    "print('>=3 entity', len([x for x in only_self_stat if x[0]>3]))\n",
    "print('average entity', sum([x[0] for x in only_self_stat])/len(only_self_stat))\n",
    "print('max', max([x[0] for x in only_self_stat]))\n",
    "print('min', min([x[0] for x in only_self_stat]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "# Pair the topic entity with other entities in the sentence\n",
    "def extract_pairs(self_id, entities):\n",
    "    pairs = []\n",
    "    for e in entities:\n",
    "        if e!=self_id:\n",
    "            pairs.append(tuple(sorted([self_id, e])))\n",
    "    return pairs\n",
    "# Extract all pairs of entities in the sentence. The only constraint is the subject need to be an entity linked to a wikipedia page\n",
    "def extract_all_pairs(links):\n",
    "    pairs = set()\n",
    "    for link1 in links: \n",
    "        if link1[4] == 'hyper':\n",
    "            e1 = link1[0]\n",
    "            for link2 in links:\n",
    "                e2 = link2[0]\n",
    "                if e1!=e2:\n",
    "                    pairs.add(tuple(sorted([e1, e2])))\n",
    "    return list(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_sentence_only_self_with_pair = annotated_sentence_only_self\\\n",
    "                            .map(lambda x:x.update({\n",
    "                                'pairs': extract_pairs(x['wikiTitle'],x['links'][1]),\n",
    "                                'all_pairs': extract_all_pairs(x['links'][0]),\n",
    "                                'md5': hashlib.md5((x['wikiTitle']+'$$%md5%$$'+x['s']).encode()).hexdigest()\n",
    "                            }) or x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'wid': 13238808,\n",
       "  'title': 'Elihu Root House',\n",
       "  'wikiTitle': 'Elihu_Root_House',\n",
       "  'sec_i': 2,\n",
       "  'p_i': 0,\n",
       "  's_i': 0,\n",
       "  's': \"The Elihu Root House was also known as the Grant House due to the marriage between Root's daughter, Edith, and Ulysses S. Grant III.\",\n",
       "  'links': [[['Elihu_Root_House', 'Elihu Root House', 4, 20, 'hyper', 'hyper'],\n",
       "    ['Ulysses_S._Grant_III',\n",
       "     'Ulysses S. Grant III',\n",
       "     111,\n",
       "     131,\n",
       "     'hyper',\n",
       "     'hyper']],\n",
       "   ['Elihu_Root_House', 'Ulysses_S._Grant_III'],\n",
       "   [['ORG:The Elihu Root House', 'The Elihu Root House', 0, 20, 'ner', 'ORG'],\n",
       "    ['ORG:the Grant House', 'the Grant House', 39, 54, 'ner', 'ORG'],\n",
       "    [\"PERSON:Root's\", \"Root's\", 83, 89, 'ner', 'PERSON'],\n",
       "    ['PERSON:Edith', 'Edith', 100, 105, 'ner', 'PERSON'],\n",
       "    ['PERSON:Ulysses S', 'Ulysses S', 111, 120, 'ner', 'PERSON']]],\n",
       "  'pairs': [('Elihu_Root_House', 'Ulysses_S._Grant_III')],\n",
       "  'all_pairs': [('Elihu_Root_House', 'Ulysses_S._Grant_III')],\n",
       "  'md5': '6c67c3cb88f8d530ce3693587a0bcb71'},\n",
       " {'wid': 16096622,\n",
       "  'title': 'Aluminium-26',\n",
       "  'wikiTitle': 'Aluminium-26',\n",
       "  'sec_i': 3,\n",
       "  'p_i': 4,\n",
       "  's_i': 3,\n",
       "  's': 'Clear evidence of the presence of 26Al at an abundance ratio of 5×10−5  was shown by Lee, et al.',\n",
       "  'links': [[['Aluminium-26', '26Al', 34, 38, 'hyper', 'hyper'],\n",
       "    ['CARDINAL:5×10−5', '5×10−5', 64, 70, 'ner', 'CARDINAL']],\n",
       "   ['Aluminium-26', 'CARDINAL:5×10−5'],\n",
       "   [['CARDINAL:26Al', '26Al', 34, 38, 'ner', 'CARDINAL'],\n",
       "    ['CARDINAL:5×10−5', '5×10−5', 64, 70, 'ner', 'CARDINAL'],\n",
       "    ['PERSON:Lee', 'Lee', 85, 88, 'ner', 'PERSON'],\n",
       "    ['PERSON:al', 'al', 93, 95, 'ner', 'PERSON']]],\n",
       "  'pairs': [('Aluminium-26', 'CARDINAL:5×10−5')],\n",
       "  'all_pairs': [('Aluminium-26', 'CARDINAL:5×10−5')],\n",
       "  'md5': '8e7fd4e2d90520a472ac0dffa56c35b8'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated_sentence_only_self_with_pair.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 496 ms, sys: 183 ms, total: 679 ms\n",
      "Wall time: 2min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "annotated_sentence_only_self_with_pair.map(lambda x:json.dumps(x)).saveAsTextFile('/data/deng.595/workspace/hybrid_pretrain/data/annotated_sentences_onlyself_withpairs.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_sentence_only_self_with_pair = sc.textFile('/data/deng.595/workspace/hybrid_pretrain/data/annotated_sentences_onlyself_withpairs.json/*')\\\n",
    "                .map(json.loads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only sentence ids and entities for join\n",
    "s_id_with_pairs = spark.createDataFrame(annotated_sentence_only_self_with_pair\\\n",
    "                    .map(lambda x:Row(pairs=x['pairs'],all_pairs=x['all_pairs'],s_id=x['md5'])))\\\n",
    "                    .groupBy('s_id').agg(F.first('pairs', True).alias('pairs'), F.first('all_pairs', True).alias('all_pairs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All posssible combinations of entities and sentence\n",
    "pair_with_sentence_id = s_id_with_pairs.select('s_id', F.explode('pairs').alias('pair'))\n",
    "allpair_with_sentence_id = s_id_with_pairs.select('s_id', F.explode('all_pairs').alias('pair'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48370096\n",
      "116766231\n",
      "CPU times: user 1.45 s, sys: 573 ms, total: 2.02 s\n",
      "Wall time: 4min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(pair_with_sentence_id.count())\n",
    "print(allpair_with_sentence_id.count())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each entity pair, group all sentences that mention it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_with_sentence_ids = pair_with_sentence_id\\\n",
    "    .groupBy('pair').agg(F.collect_set('s_id').alias('s_ids'))\\\n",
    "    .where(F.size('s_ids')>1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "allpair_with_sentence_ids = allpair_with_sentence_id\\\n",
    "    .groupBy('pair').agg(F.collect_set('s_id').alias('s_ids'))\\\n",
    "    .where(F.size('s_ids')>1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|       size(s_ids)|\n",
      "+-------+------------------+\n",
      "|  count|           2918453|\n",
      "|   mean|2.3459713074015585|\n",
      "| stddev|1.1370230766853102|\n",
      "|    min|                 2|\n",
      "|    25%|                 2|\n",
      "|    50%|                 2|\n",
      "|    75%|                 2|\n",
      "|    max|               170|\n",
      "+-------+------------------+\n",
      "\n",
      "CPU times: user 1.9 s, sys: 1.04 s, total: 2.93 s\n",
      "Wall time: 16min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pair_with_sentence_ids.select(F.size('s_ids')).summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|      size(s_ids)|\n",
      "+-------+-----------------+\n",
      "|  count|          9677194|\n",
      "|   mean|3.929200551316838|\n",
      "| stddev|39.57423584789655|\n",
      "|    min|                2|\n",
      "|    25%|                2|\n",
      "|    50%|                2|\n",
      "|    75%|                3|\n",
      "|    max|            35795|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "allpair_with_sentence_ids.select(F.size('s_ids')).summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove pair with over 2000 support sentence to prevent explosion\n",
    "allpair_with_sentence_id_noskew = allpair_with_sentence_id\\\n",
    "    .groupBy('pair').agg(F.collect_set('s_id').alias('s_ids'))\\\n",
    "    .where(F.size('s_ids')<=2000)\\\n",
    "    .select('pair', F.explode('s_ids').alias('s_id'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find potential supporting evidence for each query sentence as described in the paper.\n",
    "- `s1` is the query sentence, which can be any sentence with entity pairs.\n",
    "- `s2` is supporting evidence, it must contain the topic entity, thus more likely to contain relational knowledge.\n",
    "\n",
    "For each sentence, we get all entity pairs in it, and then join with all other sentences that mention each entity pair. And finally group all to get the potential supporting evidence set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_pairs_noskew = allpair_with_sentence_id_noskew.withColumnRenamed('s_id','s1_id')\\\n",
    "    .join(pair_with_sentence_id.withColumnRenamed('s_id','s2_id'), on='pair', how='inner')\\\n",
    "    .where(F.col('s1_id')!=F.col('s2_id'))\\\n",
    "    .groupBy(\"s1_id\",\"pair\").agg(F.collect_list(\"s2_id\").alias('s2_ids'))\\\n",
    "    .select('s1_id', F.struct('pair', 's2_ids').alias('pair_s2_ids'))\\\n",
    "    .groupBy(\"s1_id\").agg(F.collect_list(\"pair_s2_ids\").alias('pairs_s2_ids'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.88 s, sys: 1.8 s, total: 5.68 s\n",
      "Wall time: 16min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "s_pairs_noskew.write.json('/data/deng.595/workspace/hybrid_pretrain/data/annotated_sentences_onlyself_spairs_noskew.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_pairs_noskew = spark.read.json('/data/deng.595/workspace/hybrid_pretrain/data/annotated_sentences_onlyself_spairs_noskew.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(pairs_s2_ids=[Row(pair=['CARDINAL:3', 'Nimslo'], s2_ids=['52265cab682168afdce6c35fcc8b41d8'])], s1_id='001a10229229a3585868ed898e91a2fe')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_pairs_noskew.rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pairs_s2_ids: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- pair: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- s2_ids: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |-- s1_id: string (nullable = true)\n",
      "\n",
      "+-------+------------------+\n",
      "|summary|size(pairs_s2_ids)|\n",
      "+-------+------------------+\n",
      "|  count|           7631819|\n",
      "|   mean|2.1242102308768067|\n",
      "| stddev| 3.582983480542634|\n",
      "|    min|                 1|\n",
      "|    25%|                 1|\n",
      "|    50%|                 1|\n",
      "|    75%|                 2|\n",
      "|    max|              1100|\n",
      "+-------+------------------+\n",
      "\n",
      "+-------+------------------+\n",
      "|summary|size(pairs_s2_ids)|\n",
      "+-------+------------------+\n",
      "|  count|           3248827|\n",
      "|   mean|3.6408820783624365|\n",
      "| stddev|  5.11388869323814|\n",
      "|    min|                 2|\n",
      "|    25%|                 2|\n",
      "|    50%|                 3|\n",
      "|    75%|                 4|\n",
      "|    max|              1100|\n",
      "+-------+------------------+\n",
      "\n",
      "CPU times: user 16.2 ms, sys: 478 µs, total: 16.7 ms\n",
      "Wall time: 12.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "s_pairs_noskew.printSchema()\n",
    "s_pairs_noskew.select(F.size('pairs_s2_ids')).summary().show()\n",
    "s_pairs_noskew.where(F.size('pairs_s2_ids')>1).select(F.size('pairs_s2_ids')).summary().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache sentencce information to construct the final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_used_s_ids = s_pairs_noskew.select(F.explode('pairs_s2_ids.s2_ids')).select(F.explode('col')).distinct()\\\n",
    "    .union(s_pairs_noskew.select('s1_id').distinct()).distinct().rdd.map(lambda x:(x.col, None)).persist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the context for each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linked_sentences = sc.textFile('/data/deng.595/workspace/hybrid_pretrain/data/sentences_with_link_cleaned_nonempty.json/part-*')\\\n",
    "    .map(json.loads).map(lambda x:((x['title'], x['sec_i'], x['p_i']),\\\n",
    "                                   [{'s_i':x['s_i'], 's':x['linked_sentence'][0], 'md5':x['md5']}]))\n",
    "def create_sentence_with_context(paragraph):\n",
    "    paragraph.sort(key=lambda x:x['s_i'])\n",
    "    sentences_with_context = []\n",
    "    estimate_sentence_length = [len(s['s'].split()) for s in paragraph]\n",
    "    total_i = len(paragraph)\n",
    "    for i,s in enumerate(paragraph):\n",
    "        start = 0\n",
    "        s_with_context = s['s']\n",
    "        prefix_i = i-1\n",
    "        append_i = i+1\n",
    "        current_len = estimate_sentence_length[i]\n",
    "        while current_len<500 and (prefix_i>=0 or append_i<total_i):\n",
    "            if prefix_i<0:\n",
    "                p_or_a = True\n",
    "            elif append_i==total_i:\n",
    "                p_or_a = False\n",
    "            else:\n",
    "                p_or_a = random.random()>0.5 #prefix or append as context\n",
    "            if p_or_a: #append next sentence\n",
    "                sentence_to_add = paragraph[append_i]['s']\n",
    "                s_with_context = s_with_context + ' ' + sentence_to_add\n",
    "                current_len += estimate_sentence_length[append_i]\n",
    "                append_i += 1\n",
    "            else:\n",
    "                sentence_to_add = paragraph[prefix_i]['s']\n",
    "                s_with_context = sentence_to_add + ' ' + s_with_context\n",
    "                current_len += estimate_sentence_length[prefix_i]\n",
    "                prefix_i -= 1\n",
    "                start += len(sentence_to_add)+1\n",
    "        sentences_with_context.append({\n",
    "            's_with_context': s_with_context,\n",
    "            'md5': s['md5'],\n",
    "            'start': start,\n",
    "            'end': start+len(s['s'])})\n",
    "    return sentences_with_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linked_sentences_with_context = linked_sentences.reduceByKey(add).flatMap(lambda x:create_sentence_with_context(x[1]))\n",
    "linked_sentences_with_context.map(json.dumps).saveAsTextFile('/data/deng.595/workspace/hybrid_pretrain/data/sentences_with_context.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_with_context = sc.textFile('/data/deng.595/workspace/hybrid_pretrain/data/sentences_with_context.json')\\\n",
    "    .map(json.loads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_meta(meta1, meta2):\n",
    "    links = {}\n",
    "    for link in meta2['links'][0]:\n",
    "        if link[0] in links:\n",
    "            links[link[0]].append([link[2], link[3]])\n",
    "        else:\n",
    "            links[link[0]] = [[link[2], link[3]]]\n",
    "    s_start = meta1['start']\n",
    "    links_with_context = {k:[[s+s_start,e+s_start] for s,e in v] for k,v in links.items()}\n",
    "    return {\n",
    "        'md5': meta2['md5'],\n",
    "        'title': meta2['title'],\n",
    "        's': {'text':meta2['s'], 'links': links},\n",
    "        's_with_context': \\\n",
    "            {'text':meta1['s_with_context'], 's_loc':[meta1['start'],meta1['end']], 'links': links_with_context}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_used_sentences = all_used_s_ids.join(sentences_with_context.map(lambda x:(x['md5'],x)))\\\n",
    "    .join(annotated_sentence_only_self_with_pair.map(lambda x:(x['md5'],x)))\\\n",
    "    .map(lambda x:merge_meta(x[1][0][1], x[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_used_sentences.map(json.dumps).saveAsTextFile('/data/deng.595/workspace/hybrid_pretrain/data/all_used_sentences.json')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(x):\n",
    "    tokenized_s = tokenizer(x['s']['text'], add_special_tokens=False)\n",
    "    tokenized_s_with_context = tokenizer(x['s_with_context']['text'], add_special_tokens=False)\n",
    "#     try:\n",
    "    s_links = {}\n",
    "    for k,v in x['s']['links'].items():\n",
    "        anchors = []\n",
    "        for start,end in v:\n",
    "            if start<end and end>0:\n",
    "                start = tokenized_s.char_to_token(start)\n",
    "                end = tokenized_s.char_to_token(end-1)\n",
    "                if start is not None and end is not None:\n",
    "                    anchors.append([start,end+1])\n",
    "        if anchors:\n",
    "            s_links[k] = anchors\n",
    "    s_with_context_links = {}\n",
    "    for k,v in x['s_with_context']['links'].items():\n",
    "        anchors = []\n",
    "        for start,end in v:\n",
    "            if start<end and end>0:\n",
    "                start = tokenized_s_with_context.char_to_token(start)\n",
    "                end = tokenized_s_with_context.char_to_token(end-1)\n",
    "                if start is not None and end is not None:\n",
    "                    anchors.append([start,end+1])\n",
    "        if anchors:\n",
    "            s_with_context_links[k] = anchors\n",
    "    s_loc_start = x['s_with_context']['s_loc'][0]\n",
    "    while tokenized_s_with_context.char_to_token(s_loc_start) is None:\n",
    "        s_loc_start += 1\n",
    "    s_loc_start = tokenized_s_with_context.char_to_token(s_loc_start)\n",
    "    s_loc_end = x['s_with_context']['s_loc'][1]-1\n",
    "    while tokenized_s_with_context.char_to_token(s_loc_end) is None:\n",
    "        s_loc_end -= 1\n",
    "    s_loc_end = tokenized_s_with_context.char_to_token(s_loc_end)\n",
    "    if s_loc_start>=s_loc_end:\n",
    "        error = 'sentence not in context'\n",
    "    else:\n",
    "        error = 'none'\n",
    "    return {\n",
    "    'md5': x['md5'],\n",
    "    'title': x['title'],\n",
    "    's': {'ids':tokenized_s['input_ids'], 'links': s_links},\n",
    "    's_with_context': \\\n",
    "        {'ids':tokenized_s_with_context['input_ids'],\\\n",
    "         's_loc':[s_loc_start,\\\n",
    "                 s_loc_end+1],\\\n",
    "         'links': s_with_context_links},\n",
    "    'error': error\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_used_sentences = sc.textFile('/data/deng.595/workspace/hybrid_pretrain/data/all_used_sentences.json')\\\n",
    "    .map(json.loads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'md5': '4a50690d895a0b41b72426b5b0505ae9',\n",
       "  'title': 'Chicago',\n",
       "  's': {'text': 'Chicago is also a prominent center of the Polish Cathedral style of church architecture.',\n",
       "   'links': {'Chicago': [[0, 7]],\n",
       "    'Polish_Cathedral_style': [[42, 64]],\n",
       "    'church_architecture': [[68, 87]]}},\n",
       "  's_with_context': {'text': \"Chicago gave its name to the Chicago School and was home to the Prairie School, two movements in architecture. Multiple kinds and scales of houses, townhouses, condominiums, and apartment buildings can be found throughout Chicago. Large swaths of the city's residential areas away from the lake are characterized by brick bungalows built from the early 20th century through the end of World War II. Chicago is also a prominent center of the Polish Cathedral style of church architecture. The Chicago suburb of Oak Park was home to famous architect Frank Lloyd Wright, who had designed The Robie House located near the University of Chicago.\",\n",
       "   's_loc': [399, 487],\n",
       "   'links': {'Chicago': [[399, 406]],\n",
       "    'Polish_Cathedral_style': [[441, 463]],\n",
       "    'church_architecture': [[467, 486]]}}}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_used_sentences.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_used_sentences_local = dict(all_used_sentences.map(lambda x:(x['md5'],x)).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_pairs_noskew_local = dict(s_pairs_noskew.rdd.map(lambda x:(x.s1_id, x)).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(pairs_s2_ids=[Row(pair=['CARDINAL:3', 'Nimslo'], s2_ids=['52265cab682168afdce6c35fcc8b41d8'])], s1_id='001a10229229a3585868ed898e91a2fe')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_pairs_noskew_local[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webdataset as wds\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000000.tar 0 0.0 GB 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b8c5870db1a4a33add15297c5121945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7631819.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000001.tar 10000 0.1 GB 10000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000002.tar 10000 0.1 GB 20000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000003.tar 10000 0.1 GB 30000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000004.tar 10000 0.1 GB 40000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000005.tar 10000 0.1 GB 50000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000006.tar 10000 0.1 GB 60000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000007.tar 10000 0.1 GB 70000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000008.tar 10000 0.1 GB 80000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000009.tar 10000 0.1 GB 90000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000010.tar 10000 0.1 GB 100000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000011.tar 10000 0.1 GB 110000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000012.tar 10000 0.1 GB 120000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000013.tar 10000 0.1 GB 130000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000014.tar 10000 0.1 GB 140000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000015.tar 10000 0.1 GB 150000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000016.tar 10000 0.1 GB 160000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000017.tar 10000 0.1 GB 170000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000018.tar 10000 0.1 GB 180000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000019.tar 10000 0.1 GB 190000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000020.tar 10000 0.1 GB 200000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000021.tar 10000 0.1 GB 210000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000022.tar 10000 0.1 GB 220000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000023.tar 10000 0.1 GB 230000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000024.tar 10000 0.1 GB 240000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000025.tar 10000 0.1 GB 250000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000026.tar 10000 0.1 GB 260000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000027.tar 10000 0.1 GB 270000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000028.tar 10000 0.1 GB 280000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000029.tar 10000 0.1 GB 290000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000030.tar 10000 0.1 GB 300000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000031.tar 10000 0.1 GB 310000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000032.tar 10000 0.1 GB 320000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000033.tar 10000 0.1 GB 330000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000034.tar 10000 0.1 GB 340000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000035.tar 10000 0.1 GB 350000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000036.tar 10000 0.1 GB 360000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000037.tar 10000 0.1 GB 370000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000038.tar 10000 0.1 GB 380000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000039.tar 10000 0.1 GB 390000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000040.tar 10000 0.1 GB 400000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000041.tar 10000 0.1 GB 410000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000042.tar 10000 0.1 GB 420000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000043.tar 10000 0.1 GB 430000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000044.tar 10000 0.1 GB 440000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000045.tar 10000 0.1 GB 450000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000046.tar 10000 0.1 GB 460000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000047.tar 10000 0.1 GB 470000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000048.tar 10000 0.1 GB 480000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000049.tar 10000 0.1 GB 490000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000050.tar 10000 0.1 GB 500000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000051.tar 10000 0.1 GB 510000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000052.tar 10000 0.1 GB 520000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000053.tar 10000 0.1 GB 530000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000054.tar 10000 0.1 GB 540000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000055.tar 10000 0.1 GB 550000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000056.tar 10000 0.1 GB 560000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000057.tar 10000 0.1 GB 570000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000058.tar 10000 0.1 GB 580000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000059.tar 10000 0.1 GB 590000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000060.tar 10000 0.1 GB 600000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000061.tar 10000 0.1 GB 610000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000062.tar 10000 0.1 GB 620000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000063.tar 10000 0.1 GB 630000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000064.tar 10000 0.1 GB 640000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000065.tar 10000 0.1 GB 650000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000066.tar 10000 0.1 GB 660000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000067.tar 10000 0.1 GB 670000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000068.tar 10000 0.1 GB 680000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000069.tar 10000 0.1 GB 690000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000070.tar 10000 0.1 GB 700000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000071.tar 10000 0.1 GB 710000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000072.tar 10000 0.1 GB 720000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000073.tar 10000 0.1 GB 730000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000074.tar 10000 0.1 GB 740000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000075.tar 10000 0.1 GB 750000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000076.tar 10000 0.1 GB 760000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000077.tar 10000 0.1 GB 770000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000078.tar 10000 0.1 GB 780000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000079.tar 10000 0.1 GB 790000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000080.tar 10000 0.1 GB 800000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000081.tar 10000 0.1 GB 810000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000082.tar 10000 0.1 GB 820000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000083.tar 10000 0.1 GB 830000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000084.tar 10000 0.1 GB 840000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000085.tar 10000 0.1 GB 850000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000086.tar 10000 0.1 GB 860000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000087.tar 10000 0.1 GB 870000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000088.tar 10000 0.1 GB 880000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000089.tar 10000 0.1 GB 890000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000090.tar 10000 0.1 GB 900000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000091.tar 10000 0.1 GB 910000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000092.tar 10000 0.1 GB 920000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000093.tar 10000 0.1 GB 930000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000094.tar 10000 0.1 GB 940000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000095.tar 10000 0.1 GB 950000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000096.tar 10000 0.1 GB 960000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000097.tar 10000 0.1 GB 970000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000098.tar 10000 0.1 GB 980000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000099.tar 10000 0.1 GB 990000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000100.tar 10000 0.1 GB 1000000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000101.tar 10000 0.1 GB 1010000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000102.tar 10000 0.1 GB 1020000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000103.tar 10000 0.1 GB 1030000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000104.tar 10000 0.1 GB 1040000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000105.tar 10000 0.1 GB 1050000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000106.tar 10000 0.1 GB 1060000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000107.tar 10000 0.1 GB 1070000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000108.tar 10000 0.1 GB 1080000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000109.tar 10000 0.1 GB 1090000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000110.tar 10000 0.1 GB 1100000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000111.tar 10000 0.1 GB 1110000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000112.tar 10000 0.1 GB 1120000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000113.tar 10000 0.1 GB 1130000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000114.tar 10000 0.1 GB 1140000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000115.tar 10000 0.1 GB 1150000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000116.tar 10000 0.1 GB 1160000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000117.tar 10000 0.1 GB 1170000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000118.tar 10000 0.1 GB 1180000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000119.tar 10000 0.1 GB 1190000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000120.tar 10000 0.1 GB 1200000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000121.tar 10000 0.1 GB 1210000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000122.tar 10000 0.1 GB 1220000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000123.tar 10000 0.1 GB 1230000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000124.tar 10000 0.1 GB 1240000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000125.tar 10000 0.1 GB 1250000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000126.tar 10000 0.1 GB 1260000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000127.tar 10000 0.1 GB 1270000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000128.tar 10000 0.1 GB 1280000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000129.tar 10000 0.1 GB 1290000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000130.tar 10000 0.1 GB 1300000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000131.tar 10000 0.1 GB 1310000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000132.tar 10000 0.1 GB 1320000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000133.tar 10000 0.1 GB 1330000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000134.tar 10000 0.1 GB 1340000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000135.tar 10000 0.1 GB 1350000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000136.tar 10000 0.1 GB 1360000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000137.tar 10000 0.1 GB 1370000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000138.tar 10000 0.1 GB 1380000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000139.tar 10000 0.1 GB 1390000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000140.tar 10000 0.1 GB 1400000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000141.tar 10000 0.1 GB 1410000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000142.tar 10000 0.1 GB 1420000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000143.tar 10000 0.1 GB 1430000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000144.tar 10000 0.1 GB 1440000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000145.tar 10000 0.1 GB 1450000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000146.tar 10000 0.1 GB 1460000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000147.tar 10000 0.1 GB 1470000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000148.tar 10000 0.1 GB 1480000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000149.tar 10000 0.1 GB 1490000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000150.tar 10000 0.1 GB 1500000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000151.tar 10000 0.1 GB 1510000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000152.tar 10000 0.1 GB 1520000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000153.tar 10000 0.1 GB 1530000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000154.tar 10000 0.1 GB 1540000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000155.tar 10000 0.1 GB 1550000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000156.tar 10000 0.1 GB 1560000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000157.tar 10000 0.1 GB 1570000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000158.tar 10000 0.1 GB 1580000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000159.tar 10000 0.1 GB 1590000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000160.tar 10000 0.1 GB 1600000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000161.tar 10000 0.1 GB 1610000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000162.tar 10000 0.1 GB 1620000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000163.tar 10000 0.1 GB 1630000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000164.tar 10000 0.1 GB 1640000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000165.tar 10000 0.1 GB 1650000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000166.tar 10000 0.1 GB 1660000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000167.tar 10000 0.1 GB 1670000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000168.tar 10000 0.1 GB 1680000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000169.tar 10000 0.1 GB 1690000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000170.tar 10000 0.1 GB 1700000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000171.tar 10000 0.1 GB 1710000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000172.tar 10000 0.1 GB 1720000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000173.tar 10000 0.1 GB 1730000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000174.tar 10000 0.1 GB 1740000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000175.tar 10000 0.1 GB 1750000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000176.tar 10000 0.1 GB 1760000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000177.tar 10000 0.1 GB 1770000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000178.tar 10000 0.1 GB 1780000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000179.tar 10000 0.1 GB 1790000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000180.tar 10000 0.1 GB 1800000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000181.tar 10000 0.1 GB 1810000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000182.tar 10000 0.1 GB 1820000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000183.tar 10000 0.1 GB 1830000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000184.tar 10000 0.1 GB 1840000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000185.tar 10000 0.1 GB 1850000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000186.tar 10000 0.1 GB 1860000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000187.tar 10000 0.1 GB 1870000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000188.tar 10000 0.1 GB 1880000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000189.tar 10000 0.1 GB 1890000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000190.tar 10000 0.1 GB 1900000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000191.tar 10000 0.1 GB 1910000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000192.tar 10000 0.1 GB 1920000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000193.tar 10000 0.1 GB 1930000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000194.tar 10000 0.1 GB 1940000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000195.tar 10000 0.1 GB 1950000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000196.tar 10000 0.1 GB 1960000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000197.tar 10000 0.1 GB 1970000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000198.tar 10000 0.1 GB 1980000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000199.tar 10000 0.1 GB 1990000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000200.tar 10000 0.1 GB 2000000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000201.tar 10000 0.1 GB 2010000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000202.tar 10000 0.1 GB 2020000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000203.tar 10000 0.1 GB 2030000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000204.tar 10000 0.1 GB 2040000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000205.tar 10000 0.1 GB 2050000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000206.tar 10000 0.1 GB 2060000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000207.tar 10000 0.1 GB 2070000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000208.tar 10000 0.1 GB 2080000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000209.tar 10000 0.1 GB 2090000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000210.tar 10000 0.1 GB 2100000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000211.tar 10000 0.1 GB 2110000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000212.tar 10000 0.1 GB 2120000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000213.tar 10000 0.1 GB 2130000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000214.tar 10000 0.1 GB 2140000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000215.tar 10000 0.1 GB 2150000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000216.tar 10000 0.1 GB 2160000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000217.tar 10000 0.1 GB 2170000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000218.tar 10000 0.1 GB 2180000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000219.tar 10000 0.1 GB 2190000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000220.tar 10000 0.1 GB 2200000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000221.tar 10000 0.1 GB 2210000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000222.tar 10000 0.1 GB 2220000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000223.tar 10000 0.1 GB 2230000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000224.tar 10000 0.1 GB 2240000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000225.tar 10000 0.1 GB 2250000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000226.tar 10000 0.1 GB 2260000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000227.tar 10000 0.1 GB 2270000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000228.tar 10000 0.1 GB 2280000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000229.tar 10000 0.1 GB 2290000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000230.tar 10000 0.1 GB 2300000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000231.tar 10000 0.1 GB 2310000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000232.tar 10000 0.1 GB 2320000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000233.tar 10000 0.1 GB 2330000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000234.tar 10000 0.1 GB 2340000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000235.tar 10000 0.1 GB 2350000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000236.tar 10000 0.1 GB 2360000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000237.tar 10000 0.1 GB 2370000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000238.tar 10000 0.1 GB 2380000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000239.tar 10000 0.1 GB 2390000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000240.tar 10000 0.1 GB 2400000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000241.tar 10000 0.1 GB 2410000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000242.tar 10000 0.1 GB 2420000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000243.tar 10000 0.1 GB 2430000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000244.tar 10000 0.1 GB 2440000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000245.tar 10000 0.1 GB 2450000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000246.tar 10000 0.1 GB 2460000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000247.tar 10000 0.1 GB 2470000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000248.tar 10000 0.1 GB 2480000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000249.tar 10000 0.1 GB 2490000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000250.tar 10000 0.1 GB 2500000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000251.tar 10000 0.1 GB 2510000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000252.tar 10000 0.1 GB 2520000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000253.tar 10000 0.1 GB 2530000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000254.tar 10000 0.1 GB 2540000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000255.tar 10000 0.1 GB 2550000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000256.tar 10000 0.1 GB 2560000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000257.tar 10000 0.1 GB 2570000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000258.tar 10000 0.1 GB 2580000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000259.tar 10000 0.1 GB 2590000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000260.tar 10000 0.1 GB 2600000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000261.tar 10000 0.1 GB 2610000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000262.tar 10000 0.1 GB 2620000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000263.tar 10000 0.1 GB 2630000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000264.tar 10000 0.1 GB 2640000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000265.tar 10000 0.1 GB 2650000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000266.tar 10000 0.1 GB 2660000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000267.tar 10000 0.1 GB 2670000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000268.tar 10000 0.1 GB 2680000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000269.tar 10000 0.1 GB 2690000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000270.tar 10000 0.1 GB 2700000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000271.tar 10000 0.1 GB 2710000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000272.tar 10000 0.1 GB 2720000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000273.tar 10000 0.1 GB 2730000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000274.tar 10000 0.1 GB 2740000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000275.tar 10000 0.1 GB 2750000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000276.tar 10000 0.1 GB 2760000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000277.tar 10000 0.1 GB 2770000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000278.tar 10000 0.1 GB 2780000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000279.tar 10000 0.1 GB 2790000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000280.tar 10000 0.1 GB 2800000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000281.tar 10000 0.1 GB 2810000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000282.tar 10000 0.1 GB 2820000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000283.tar 10000 0.1 GB 2830000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000284.tar 10000 0.1 GB 2840000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000285.tar 10000 0.1 GB 2850000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000286.tar 10000 0.1 GB 2860000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000287.tar 10000 0.1 GB 2870000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000288.tar 10000 0.1 GB 2880000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000289.tar 10000 0.1 GB 2890000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000290.tar 10000 0.1 GB 2900000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000291.tar 10000 0.1 GB 2910000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000292.tar 10000 0.1 GB 2920000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000293.tar 10000 0.1 GB 2930000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000294.tar 10000 0.1 GB 2940000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000295.tar 10000 0.1 GB 2950000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000296.tar 10000 0.1 GB 2960000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000297.tar 10000 0.1 GB 2970000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000298.tar 10000 0.1 GB 2980000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000299.tar 10000 0.1 GB 2990000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000300.tar 10000 0.1 GB 3000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000301.tar 10000 0.1 GB 3010000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000302.tar 10000 0.1 GB 3020000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000303.tar 10000 0.1 GB 3030000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000304.tar 10000 0.1 GB 3040000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000305.tar 10000 0.1 GB 3050000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000306.tar 10000 0.1 GB 3060000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000307.tar 10000 0.1 GB 3070000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000308.tar 10000 0.1 GB 3080000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000309.tar 10000 0.1 GB 3090000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000310.tar 10000 0.1 GB 3100000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000311.tar 10000 0.1 GB 3110000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000312.tar 10000 0.1 GB 3120000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000313.tar 10000 0.1 GB 3130000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000314.tar 10000 0.1 GB 3140000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000315.tar 10000 0.1 GB 3150000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000316.tar 10000 0.1 GB 3160000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000317.tar 10000 0.1 GB 3170000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000318.tar 10000 0.1 GB 3180000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000319.tar 10000 0.1 GB 3190000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000320.tar 10000 0.1 GB 3200000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000321.tar 10000 0.1 GB 3210000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000322.tar 10000 0.1 GB 3220000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000323.tar 10000 0.1 GB 3230000\n",
      "# writing /data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/000324.tar 10000 0.1 GB 3240000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# No tokenization version\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "import copy\n",
    "overlap_errors = set()\n",
    "with wds.ShardWriter('/data/deng.595/workspace/hybrid_pretrain/data/sentence_multi_pairs_for_pretrain_no_tokenization/%06d.tar', maxcount=10000) as sink:\n",
    "    for i, sample in enumerate(tqdm(s_pairs_noskew_local)):\n",
    "        s1 = all_used_sentences_local.get(sample.s1_id, None)\n",
    "        if s1 is None:\n",
    "            print(f'error not found: {i}, {sample.s1_id}')\n",
    "            continue\n",
    "        s1 = s1['s']\n",
    "        s1_text = s1['text']\n",
    "        s1_links_and_anchors = {}\n",
    "        for k, anchors in s1['links'].items():\n",
    "            anchor_patterns = set()\n",
    "            for anchor in anchors:\n",
    "                anchor = s1['text'][anchor[0]:anchor[1]]\n",
    "                if anchor.strip()!='':\n",
    "                    anchor_patterns.add(anchor)\n",
    "            if len(anchor_patterns)>0:\n",
    "                s1_links_and_anchors[k] = [re.compile(r'\\b'+re.escape(pattern)+r'\\b') for pattern in anchor_patterns]\n",
    "        pairs_data = []\n",
    "        for pair_s2 in sample.pairs_s2_ids:\n",
    "            pair = pair_s2.pair\n",
    "            s2s = []\n",
    "            for s2_id in pair_s2.s2_ids:\n",
    "                s2 = all_used_sentences_local.get(s2_id, None)\n",
    "                if s2 is None:\n",
    "                    print(f'error not found: {i}, {s2_id}')\n",
    "                    continue\n",
    "                s2 = copy.deepcopy(s2['s_with_context'])\n",
    "                s2_text = s2['text']\n",
    "                if s1_text in s2_text:\n",
    "                    s1_length = len(s1_text)\n",
    "                    s1_in_s2_start = s2_text.find(s1_text)\n",
    "                    s1_in_s2_end = s1_length\n",
    "                    if s1_in_s2_start < s2['s_loc'][1]:\n",
    "                        if s1_in_s2_end>s2['s_loc'][0]:\n",
    "                            overlap_errors.add((i,s2_id))\n",
    "                            continue\n",
    "                        else:\n",
    "                            s2['text'] = s2_text[:s1_in_s2_start]+s2_text[s1_in_s2_end:]\n",
    "                            s2['s_loc'] = [s2['s_loc'][0]-s1_length, s2['s_loc'][1]-s1_length]\n",
    "                            for k in s2['links']:\n",
    "                                s2['links'][k] = [[anchor[0]-s1_length, anchor[1]-s1_length] for anchor in s2['links'][k]]\n",
    "                anchors0 = s2['links'].get(pair[0],[])\n",
    "                anchors1 = s2['links'].get(pair[1],[])\n",
    "                for k, patterns in s1_links_and_anchors.items():\n",
    "                    new_anchors = [[match.start(),match.end()] for pattern in patterns for match in pattern.finditer(s2['text'])]\n",
    "                    new_anchors = [[start, end] for start, end in new_anchors if end<=s2['s_loc'][0] or start>=s2['s_loc'][1]]\n",
    "                    if new_anchors:\n",
    "                        if k in s2['links']:\n",
    "                            s2['links'][k] += new_anchors\n",
    "                        else:\n",
    "                            s2['links'][k] = new_anchors\n",
    "                if anchors0 and anchors1:\n",
    "                    s2s.append({\n",
    "                        'md5': s2_id,\n",
    "                        'text': s2['text'],\n",
    "                        's_loc': s2['s_loc'],\n",
    "                        'pair_locs': [anchors0, anchors1],\n",
    "                        'all_links': s2['links'],\n",
    "                    })\n",
    "            if s2s:\n",
    "                anchors0 = s1['links'].get(pair[0],[])\n",
    "                anchors1 = s1['links'].get(pair[1],[])\n",
    "                if anchors0 and anchors1:\n",
    "                    pairs_data.append({\n",
    "                        'pair': pair,\n",
    "                        's1_pair_locs': [anchors0, anchors1],\n",
    "                        's2s': s2s\n",
    "                    })\n",
    "        if len(pairs_data)>1:\n",
    "            sink.write({\n",
    "                '__key__': sample.s1_id,\n",
    "                'json': {\n",
    "                    's1_text': s1['text'],\n",
    "                    's1_all_links': s1['links'],\n",
    "                    'pairs': pairs_data\n",
    "                }\n",
    "            })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8 (default, Aug 13 2020, 07:46:32) \n[GCC 4.8.5 20150623 (Red Hat 4.8.5-39)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
