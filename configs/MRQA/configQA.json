{
    "name": "MRQA_roberta",
    "n_gpus": 1,
    "seed": 123,
    "arch": {
        "type": "SentencePairModelForQA",
        "args": {
            "bert_version": "roberta-base",
            "gradient_checkpointing": false
        }
    },
    "tokenizer": "roberta-base",
    "train_dataset": {
        "type": "MRQADataset",
        "args": {
            "datadir": "{Path_To_Data}",
            "split": "train",
            "sample_num": 1024,
            "neg_ratio": -1,
            "overwrite": false,
            "skip_first_line": true
        }
    },
    "eval_dataset": {
        "type": "MRQADataset",
        "args": {
            "datadir": "{Path_To_Data}",
            "split": "eval",
            "overwrite": false,
            "skip_first_line": true
        }
    },
    "collator": {
        "type": "MRQA_collate",
        "args": {
            "use_token_type": false
        }
    },
    "trainer_type": {
        "type": "BaseTrainerForQA",
        "args": {}
    },
    "trainer": {
        "num_train_epochs": 10,
        "warmup_steps": 1.0,
        "weight_decay": 0.01,
        "gradient_accumulation_steps": 2,
        "learning_rate": 5e-5,
        "max_grad_norm": 1.0,
        "logging_steps": 100,
        "save_total_limit": 2,
        "fp16": true,
        "per_device_train_batch_size": 10,
        "per_device_eval_batch_size": 64,
        "evaluation_strategy": "steps"
    },
    "save_dir": "{Path_To_Output}",
    "local_pretrained": "",
    "use_our_pretrained": 0,
    "preprocess_only": 0,
    "do_test": 0,
    "test_only": 0
}