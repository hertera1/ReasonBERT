{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nqtables data is a subset of the original Natural Questions dataset, where the answer could be found from the tables in the given wiki article. The original Natural Questions dataset could be found [here](https://ai.google.com/research/NaturalQuestions/download)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_table(tokens):\n",
    "    table_locs = [-1 for i in range(len(tokens))]\n",
    "    tids = []\n",
    "    tables = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token == '<Table>':\n",
    "            tids.append(len(tables))\n",
    "            tables.append([])\n",
    "        if tids:\n",
    "            table_locs[i] = tids[-1]\n",
    "            tables[tids[-1]].append(token)\n",
    "        if token == '</Table>':\n",
    "            tids.pop()\n",
    "    return table_locs, tables\n",
    "\n",
    "def extract_table(tokens):\n",
    "    data = []\n",
    "    row = None\n",
    "    cell = None\n",
    "    merged_cells = {}\n",
    "    colspan = 1\n",
    "    for token in tokens[1:-1]:\n",
    "        if token == '<Tr>':\n",
    "            row= []\n",
    "        elif token == '</Tr>':\n",
    "            data.append(row)\n",
    "            row = None\n",
    "        elif token[:3] in ['<Td','<Th']:\n",
    "            cell = []\n",
    "            try:\n",
    "                colspan = 1 if 'colspan' not in token else int(re.findall(r'colspan=.*?(\\d+)', token))\n",
    "            except:\n",
    "                colspan = 1\n",
    "        elif token[:4] in ['</Td','</Th']:\n",
    "            row+=[' '.join(cell)]*colspan\n",
    "            cell = None\n",
    "        else:\n",
    "            if cell is None:\n",
    "                cell = []\n",
    "            if row is None:\n",
    "                row = []\n",
    "            cell.append(token)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/original/NaturalQuestions/simplified-nq-train.jsonl', 'r') as f_in:\n",
    "    questions = []\n",
    "    short_answer_in_table_questions = []\n",
    "    long_answer_in_table_questions = []\n",
    "    for idx, line in enumerate(tqdm(f_in)):\n",
    "        sample = json.loads(line)\n",
    "        raw_document_text = sample['document_text'].split(' ')\n",
    "        list_document_text = []\n",
    "        old_to_new_map = {-1:-1}\n",
    "        num_token_added = 0\n",
    "        for long_answer_candidate in sample['long_answer_candidates']:\n",
    "            if long_answer_candidate['top_level']:\n",
    "                if any([(token[0]!='<' or token[-1]!='>') for token in raw_document_text[long_answer_candidate['start_token']:long_answer_candidate['end_token']]]):\n",
    "                    for i in range(long_answer_candidate['start_token'],long_answer_candidate['end_token']+1):\n",
    "                        old_to_new_map[i] = num_token_added+i-long_answer_candidate['start_token']\n",
    "                    num_token_added += long_answer_candidate['end_token']-long_answer_candidate['start_token']\n",
    "                    list_document_text.append(raw_document_text[long_answer_candidate['start_token']:long_answer_candidate['end_token']])\n",
    "        tokens = [token for document_text in list_document_text for token in document_text]\n",
    "        document_text = ' '.join(tokens)\n",
    "        table_locs, tables = find_table(tokens)\n",
    "        tables = [extract_table(t) for t in tables]\n",
    "        answer_in_table = []\n",
    "        long_answer_in_table = []\n",
    "        has_long = False\n",
    "        has_short = False\n",
    "        has_table = any([x==1 for x in table_locs])\n",
    "        \n",
    "        for annotation in sample['annotations']:\n",
    "            answer = annotation['long_answer']\n",
    "            start = old_to_new_map.get(answer['start_token'], None)\n",
    "            end = old_to_new_map.get(answer['end_token'], None)\n",
    "            if start is None or end is None:\n",
    "                print('missed long in:', idx)\n",
    "                continue\n",
    "            if answer['start_token']!=-1:\n",
    "                if all([True if table_locs[i]!=-1 else False for i in range(start, end)]):\n",
    "                    long_answer_in_table.append([table_locs[start], start, end])\n",
    "                has_long = True\n",
    "            if annotation['short_answers']:\n",
    "                has_short =True\n",
    "            for answer in annotation['short_answers']:\n",
    "                start = old_to_new_map.get(answer['start_token'], None)\n",
    "                end = old_to_new_map.get(answer['end_token'], None)\n",
    "                if start is None or end is None:\n",
    "                    print('missed short in:', idx)\n",
    "                    continue\n",
    "                if all([True if table_locs[i]!=-1 else False for i in range(start, end)]):\n",
    "                    answer_in_table.append([table_locs[start], ' '.join(tokens[start:end]), start, end])\n",
    "        if answer_in_table:\n",
    "            short_answer_in_table_questions.append({\n",
    "                'q': sample['question_text'],\n",
    "                't': tables,\n",
    "                'd': document_text,\n",
    "                'list_d': list_document_text,\n",
    "                'a': sample['annotations'],\n",
    "                'a in table': answer_in_table\n",
    "            })\n",
    "        if long_answer_in_table:\n",
    "            long_answer_in_table_questions.append({\n",
    "                'q': sample['question_text'],\n",
    "                'd': document_text,\n",
    "                'list_d': list_document_text,\n",
    "                'a': sample['annotations'],\n",
    "                'a in table': long_answer_in_table\n",
    "            })\n",
    "        questions.append([has_long, has_short, has_table, sample['question_text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_pattern(answer):\n",
    "    pattern = re.escape(answer)\n",
    "    if re.match('\\w', answer[0]):\n",
    "        pattern = r'\\b'+pattern\n",
    "    if re.match('\\w', answer[-1]):\n",
    "        pattern += r'\\b'\n",
    "    return pattern\n",
    "def find_answer_cell(answer, table):\n",
    "    loc = []\n",
    "    for i, row in enumerate(table):\n",
    "        for j, cell in enumerate(row):\n",
    "            if re.search(get_answer_pattern(answer),cell) is not None:\n",
    "                loc.append([len(cell),[i,j]])\n",
    "            \n",
    "    if loc:\n",
    "        return sorted(loc, key=lambda x:(x[0]+9999 if x[1][0]==0 else x[0]))[0][1]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "total_num = len(short_answer_in_table_questions)\n",
    "dev_num = int(0.1*total_num)\n",
    "dev_idxes = set(random.sample(list(range(total_num)),dev_num))\n",
    "train_examples = []\n",
    "dev_examples = []\n",
    "for qid, sample in enumerate(short_answer_in_table_questions):\n",
    "    if qid in dev_idxes:\n",
    "        dev_examples.append(sample)\n",
    "    else:\n",
    "        train_examples.append(sample)\n",
    "print('train:', len(train_examples))\n",
    "print('dev:', len(dev_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We experiment with two types of models. One is linearized the table and treat it as regular text QA task (MRQA), the other is using a Table QA model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for Text QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_removed = set()\n",
    "max_position = 49\n",
    "def clean_context(context):\n",
    "    p_idx = 0\n",
    "    t_idx = 0\n",
    "    l_idx = 0\n",
    "    last_tag = False\n",
    "    old_to_new_map = {}\n",
    "    cleaned_context = []\n",
    "    for i, token in enumerate(context):\n",
    "        if token == '<P>':\n",
    "            last_tag=True\n",
    "            cleaned_context.append(f'[P={p_idx}]')\n",
    "            old_to_new_map[i] = len(cleaned_context)\n",
    "            if p_idx<max_position:\n",
    "                p_idx+=1\n",
    "        elif token == '<Table>':\n",
    "            last_tag=True\n",
    "            cleaned_context.append(f'[Tab={t_idx}]')\n",
    "            old_to_new_map[i] = len(cleaned_context)\n",
    "            if t_idx<max_position:\n",
    "                t_idx+=1\n",
    "        elif token == '<Ul>':\n",
    "            last_tag=True\n",
    "            cleaned_context.append(f'[List={t_idx}]')\n",
    "            old_to_new_map[i] = len(cleaned_context)\n",
    "            if l_idx<max_position:\n",
    "                l_idx+=1\n",
    "        elif token[0]=='<' and token[-1]=='>':\n",
    "            tag_removed.add(token)\n",
    "            if not last_tag:\n",
    "                last_tag=True\n",
    "                cleaned_context.append('[SEP]')\n",
    "                old_to_new_map[i] = len(cleaned_context)\n",
    "        elif token!='':\n",
    "            last_tag=False\n",
    "            cleaned_context.append(token)\n",
    "            old_to_new_map[i] = len(cleaned_context)\n",
    "    return cleaned_context\n",
    "def prepare_examples_as_MRQA(all_examples, split='train'):\n",
    "    processed_examples = []\n",
    "    processed_examples_only_table = []\n",
    "    for qid, sample in enumerate(tqdm(all_examples)):\n",
    "        processed_example = {}\n",
    "        processed_example_only_table = {}\n",
    "        qid = f'{split}-{qid}'\n",
    "        all_answers = [a[1] for a in sample['a in table']]\n",
    "        context = [token for x in sample['list_d'][:(50 if split=='train' else 99999)] for token in x]\n",
    "        context = ' '.join(clean_context(context))\n",
    "        processed_example['context'] = context\n",
    "        table_only_context = ' '.join(clean_context([token for t in find_table(sample['d'].split(' '))[1] for token in t]))\n",
    "        processed_example_only_table['context'] = table_only_context\n",
    "        qas = {'question': sample['q'], 'qid': qid, 'answers': all_answers}\n",
    "        table_only_qas = qas.copy()\n",
    "        detected_answers = []\n",
    "        table_only_detected_answers = []\n",
    "        for a in sample['a in table']:\n",
    "            detected_answer = {}\n",
    "            detected_answer['char_spans'] = [[loc.start(), loc.end()-1] for loc in re.finditer(r'\\b'+re.escape(a[1])+r'\\b', context)]\n",
    "            if detected_answer['char_spans']:\n",
    "                detected_answer['text'] = a[1]\n",
    "                detected_answers.append(detected_answer)                \n",
    "            detected_answer_in_table = detected_answer.copy()\n",
    "            detected_answer_in_table['char_spans'] = [[loc.start(), loc.end()-1] for loc in re.finditer(r'\\b'+re.escape(a[1])+r'\\b', table_only_context)]\n",
    "            if detected_answer_in_table['char_spans']:\n",
    "                detected_answer_in_table['text'] = a[1]\n",
    "                table_only_detected_answers.append(detected_answer_in_table)\n",
    "        qas['detected_answers'] = detected_answers\n",
    "        table_only_qas['detected_answers'] = table_only_detected_answers\n",
    "        processed_example['qas'] = [qas]\n",
    "        processed_example_only_table['qas'] = [table_only_qas]\n",
    "        processed_examples.append(processed_example)\n",
    "        processed_examples_only_table.append(processed_example_only_table)\n",
    "    return processed_examples, processed_examples_only_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prepared_MRQA_examples, train_prepared_tableonly_MRQA_examples = prepare_examples_as_MRQA(train_examples, 'train')\n",
    "dev_prepared_MRQA_examples, dev_prepared_tableonly_MRQA_examples = prepare_examples_as_MRQA(dev_examples, 'dev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for Table QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_loader.table_utils as table_utils\n",
    "\n",
    "def get_values(text):\n",
    "    values = []\n",
    "    value_spans = table_utils.parse_text(text)\n",
    "    for value_span in value_spans:\n",
    "        span_index = [value_span.begin_index, value_span.end_index]\n",
    "        for value in value_span.values:\n",
    "            value_type = 'date' if value.date is not None else 'number'\n",
    "            values.append([value_type, span_index, value.float_value if value_type=='number' else (value.date.year,value.date.month,value.date.day)])\n",
    "            break\n",
    "    return values\n",
    "\n",
    "# clean cell: clean whitespaces, remove format tags\n",
    "# parse number and date, add rank\n",
    "def process_table(table, min_consolidation_fraction=0.7):\n",
    "    raw_data = table\n",
    "    processed_data = []\n",
    "    processed_values = []\n",
    "    processed_ranks = []\n",
    "    processed_inv_ranks = []\n",
    "    error = None\n",
    "    max_col_num = max([len(row) for row in raw_data])\n",
    "    for i, row in enumerate(raw_data):\n",
    "        if all([cell=='' for cell in row]):\n",
    "            continue\n",
    "        processed_row = []\n",
    "        processed_row_values = []\n",
    "        for j, cell in enumerate(row):\n",
    "            processed_row.append(cell)\n",
    "            processed_row_values.append(table_utils.parse_text(cell))\n",
    "        processed_row += ['']*(max_col_num-len(row))\n",
    "        processed_row_values += [[]]*(max_col_num-len(row))\n",
    "        processed_data.append(processed_row)\n",
    "        processed_values.append(processed_row_values)\n",
    "        processed_ranks.append([0 for _ in range(max_col_num)])\n",
    "        processed_inv_ranks.append([0 for _ in range(max_col_num)])\n",
    "    for j in range(max_col_num):\n",
    "        column_values = {i:processed_values[i][j] for i in range(1,len(processed_values))}\n",
    "        column_values = table_utils._consolidate_numeric_values(\n",
    "            column_values,\n",
    "            min_consolidation_fraction=min_consolidation_fraction,\n",
    "        )\n",
    "        try:\n",
    "            key_fn = table_utils.get_numeric_sort_key_fn([cell_value[1] for cell_value in column_values.values()])\n",
    "            column_numeric_values_to_rank = {row_index: key_fn(value[1]) for row_index, value in column_values.items()}\n",
    "\n",
    "            column_numeric_values_inv = collections.defaultdict(list)\n",
    "            for row_index, value in column_numeric_values_to_rank.items():\n",
    "                column_numeric_values_inv[value].append(row_index)\n",
    "\n",
    "            unique_values = sorted(column_numeric_values_inv.keys())\n",
    "\n",
    "            for rank, value in enumerate(unique_values):\n",
    "                for row_index in column_numeric_values_inv[value]:\n",
    "                    processed_ranks[row_index][j] = rank + 1\n",
    "                    processed_inv_ranks[row_index][j] = len(unique_values) - rank\n",
    "        except ValueError:\n",
    "            pass\n",
    "    for i in range(len(processed_values)):\n",
    "        for j in range(max_col_num):\n",
    "            values = []\n",
    "            for value_span in processed_values[i][j]:\n",
    "                span_index = [value_span.begin_index, value_span.end_index]\n",
    "                for value in value_span.values:\n",
    "                    value_type = 'date' if value.date is not None else 'number'\n",
    "                    values.append([value_type,span_index, value.float_value if value_type=='number' else (value.date.year,value.date.month,value.date.day)])\n",
    "                    break\n",
    "            processed_values[i][j] = values\n",
    "    return {\n",
    "        'text': processed_data,\n",
    "        'error': error,\n",
    "        'values': processed_values,\n",
    "        'value_ranks': processed_ranks,\n",
    "        'value_inv_ranks': processed_inv_ranks,\n",
    "        'index': [[[i,j] for j in range(len(processed_data[0]))] for i in range(len(processed_data))]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_to_remove = re.compile('<Ul>|</Li>|</Ul>')\n",
    "def prepare_examples(all_examples, split='train', max_cell = 50, max_row=400, max_table = 400):\n",
    "    def chunk_table(table, answer, tid):\n",
    "        if len(table['text'])==0:\n",
    "            return None     \n",
    "        table_text = [[cell.split()[:max_cell] for cell in row] for row in table['text']]\n",
    "\n",
    "        i_start = 1 if len(table_text)>1 else 0\n",
    "        headers = table_text[0]\n",
    "        while i_start<len(table_text):\n",
    "            tmp_table = []\n",
    "            tmp_size = len(headers[0])+len(table_text[i_start][0])\n",
    "            j_start = 1 if len(table_text[i_start])>1 else 0\n",
    "            i_end = i_start+1\n",
    "            while j_start<len(table_text[i_start]):\n",
    "                j_end = j_start\n",
    "                for j in range(j_start, len(table_text[i_start])):\n",
    "                    if tmp_size+len(headers[j])+len(table_text[i_start][j])<=max_row:\n",
    "                        j_end += 1\n",
    "                        tmp_size += len(headers[j])+len(table_text[i_start][j])\n",
    "                    else:\n",
    "                        break\n",
    "                if j_start!=0:\n",
    "                    tmp_headers = [headers[0]]+headers[j_start:j_end]\n",
    "                    tmp_table.append([table_text[i_start][0]]+table_text[i_start][j_start:j_end])\n",
    "                else:\n",
    "                    tmp_headers = headers[j_start:j_end]\n",
    "                    tmp_table.append(table_text[i_start][j_start:j_end])\n",
    "                if j_start==1 and j_end==len(table_text[i_start]):\n",
    "                    for row in table_text[i_start+1:]:\n",
    "                        row_size = sum([len(cell) for cell in row])\n",
    "                        if row_size+tmp_size<=max_table:\n",
    "                            tmp_table.append(row)\n",
    "                            i_end += 1\n",
    "                            tmp_size += row_size\n",
    "                        else:\n",
    "                            break\n",
    "                tmp_headers = [' '.join(cell) for cell in tmp_headers]\n",
    "                tmp_table = [[' '.join(cell) for cell in row] for row in tmp_table]\n",
    "                if j_start!=0:\n",
    "                    i_map = {0:0}\n",
    "                    j_map = {0:0}\n",
    "                    i_map.update({i0:i1 for i0,i1 in zip(range(1,1+i_end-i_start),range(i_start, i_end))})\n",
    "                    j_map.update({j0:j1 for j0,j1 in zip(range(1,1+j_end-j_start),range(j_start, j_end))})\n",
    "                else:\n",
    "                    i_map = {0:0}\n",
    "                    j_map = {0:0}\n",
    "                    i_map.update({i0:i1 for i0,i1 in zip(range(1,1+i_end-i_start),range(i_start, i_end))})\n",
    "                    j_map.update({j0:j1 for j0,j1 in zip(range(0,j_end),range(j_start, j_end))})\n",
    "                if answer!='':\n",
    "                    cell_index = find_answer_cell(answer, [tmp_headers]+tmp_table)\n",
    "                else:\n",
    "                    cell_index = None\n",
    "                if cell_index is None:\n",
    "                    cell_index = [-1, -1]\n",
    "                    cell_span = [-1,-1]\n",
    "                else:\n",
    "                    if cell_index[0] == 0:\n",
    "                        cell_start = re.search(get_answer_pattern(answer),tmp_headers[cell_index[1]]).start()\n",
    "                    else:\n",
    "                        cell_start = re.search(get_answer_pattern(answer),tmp_table[cell_index[0]-1][cell_index[1]]).start()\n",
    "                    cell_span = [cell_start,cell_start+len(answer)]\n",
    "                yield {\n",
    "                    'table':{\n",
    "                        'idx': tid,\n",
    "                        'data': [tmp_headers]+tmp_table,\n",
    "                        'index':[[table['index'][i_map[i]][j_map[j]] for j in range(len(j_map))] for i in range(len(i_map))],\n",
    "                        'values':[[table['values'][i_map[i]][j_map[j]] for j in range(len(j_map))] for i in range(len(i_map))],\n",
    "                        'value_ranks':[[table['value_ranks'][i_map[i]][j_map[j]] for j in range(len(j_map))] for i in range(len(i_map))],\n",
    "                        'value_inv_ranks':[[table['value_inv_ranks'][i_map[i]][j_map[j]] for j in range(len(j_map))] for i in range(len(i_map))],\n",
    "                    },\n",
    "                    'answer': {\n",
    "                        'text': answer if cell_index[0]!=-1 else '',\n",
    "                        'index': cell_index,\n",
    "                        'span': cell_span\n",
    "                    },\n",
    "                }\n",
    "                tmp_size = len(headers[0])+len(table_text[i_start][0])\n",
    "                tmp_table = []\n",
    "                if j_end == len(table_text[i_start]):\n",
    "                    break\n",
    "                else:\n",
    "                    j_start = min([j_start+4,j_end])\n",
    "                    i_end = i_start+1\n",
    "            i_start = i_end\n",
    "\n",
    "    processed_examples = []\n",
    "    missed_examples = []\n",
    "    for qid, sample in enumerate(tqdm(all_examples)):\n",
    "        qid = f'{split}-{qid}'\n",
    "        used_t = set()\n",
    "        all_answers = [re.sub(' +',' ',tags_to_remove.sub('', a[1])) for a in sample['a in table']]\n",
    "        found_answer = False\n",
    "        question = sample['q']\n",
    "        question_values = get_values(question)\n",
    "        for a in sample['a in table']:\n",
    "            if a[0] in used_t and found_answer:\n",
    "                continue\n",
    "            table = sample['t'][a[0]]\n",
    "            if len(table)==0:\n",
    "                continue\n",
    "            table = [[re.sub(' +',' ',tags_to_remove.sub('', cell)) for cell in row] for row in table]\n",
    "            table = process_table(table)\n",
    "            used_t.add(a[0])\n",
    "            answer = re.sub(' +',' ',tags_to_remove.sub('', a[1]))\n",
    "            for processed_example in chunk_table(table, answer, a[0]):\n",
    "                if processed_example is None:\n",
    "                    break\n",
    "                processed_example.update({\n",
    "                    'qid': qid,\n",
    "                    'question': question,\n",
    "                    'question_values': question_values,\n",
    "                    'all_answers': all_answers\n",
    "                })\n",
    "                processed_examples.append(processed_example)\n",
    "                if processed_example['answer']['text']!='':\n",
    "                    found_answer = True\n",
    "        if not found_answer:\n",
    "            missed_examples.append([qid, sample])\n",
    "        for tid, table in enumerate(sample['t']):\n",
    "            if tid in used_t:\n",
    "                continue\n",
    "            table = [[re.sub(' +',' ',tags_to_remove.sub('', cell)) for cell in row] for row in table]\n",
    "            if len(table)==0:\n",
    "                continue\n",
    "            table = process_table(table)\n",
    "            for processed_example in chunk_table(table, '', tid):\n",
    "                if processed_example is None:\n",
    "                    break\n",
    "                processed_example.update({\n",
    "                    'qid': qid,\n",
    "                    'question': question,\n",
    "                    'question_values': question_values,\n",
    "                    'all_answers': all_answers\n",
    "                })\n",
    "                processed_examples.append(processed_example)\n",
    "    return processed_examples, missed_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prepared_examples_row_first, train_missed_examples_row_first = prepare_examples(train_examples, 'train')\n",
    "dev_prepared_examples_row_first, dev_missed_examples_row_first = prepare_examples(dev_examples, 'dev')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
